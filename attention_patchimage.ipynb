{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch, torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import torchvision.utils as vutils\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "oWtoAcFqBwxh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2B-mNa6YBuTY"
      },
      "outputs": [],
      "source": [
        "class OurAttentionLayer(nn.Module):\n",
        "    def __init__(self, patch_size, channels_in, hidden_dim, emb_size=1, cross=False):\n",
        "        super().__init__()\n",
        "        self.cross = cross\n",
        "        self.hidden_dim = hidden_dim #N\n",
        "        self.channels_in = channels_in #C\n",
        "        self.patch_size = patch_size #pq\n",
        "        self.emb_size = emb_size #Z\n",
        "        self.Wk = nn.Linear(patch_size, hidden_dim)     # [pq, N]\n",
        "        self.Wv = nn.Linear(patch_size, hidden_dim)     # [pq, N]\n",
        "        self.LN = nn.LayerNorm([channels_in, patch_size])\n",
        "        if cross:\n",
        "            self.Wi = nn.Linear(emb_size, channels_in)  # [Z, C]\n",
        "            self.Wj = nn.Linear(emb_size, hidden_dim)   # [Z, N]\n",
        "            self.Wq = nn.Linear(hidden_dim, hidden_dim) # [N, N]\n",
        "        else:\n",
        "            self.Wq = nn.Linear(patch_size, hidden_dim) # [C, N]\n",
        "        self.Wr = nn.Linear(hidden_dim, patch_size)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dscale = 1/(hidden_dim**0.5)\n",
        "    def forward(self, image, text = None, ret_attn_QKV=False):\n",
        "        # image == [Batch, channels, patch_size] == [..., C, pq]\n",
        "        K = self.Wk(image) # [..., C, pq] * [pq, N] = [..., C, N]\n",
        "        V = self.Wv(image)\n",
        "        if self.cross and text is None:\n",
        "            text = torch.rand(1, self.emb_size)\n",
        "        if self.cross:\n",
        "            #text_T = torch.permute(text, (-1, -2))\n",
        "            # text = [Batch, seq_len, emb_size] == [..., S, Z]\n",
        "            I = self.Wi(text) # [..., S, Z] * [Z, C] -> [..., S, C]\n",
        "            J = self.Wj(text) # [..., S, Z] * [Z, N] -> [..., S, N]\n",
        "            I_T = torch.transpose(I, -1, -2) # [..., C, S]\n",
        "            Q1 = torch.einsum(\"...cs,...sn->...cn\", I_T, J) # возможно надо отдебажить учитывая Batch и прочее\n",
        "            # [..., C, S] * [..., S, N] -> C, N\n",
        "            Q = self.Wq(Q1).unsqueeze(1).expand_as(K) # -> C, N\n",
        "        else:\n",
        "            Q = self.Wq(image) # [..., C, pq] * [pq, N] = [..., C, N]\n",
        "\n",
        "        qk = torch.einsum(\"...jn,...cn->...cj\", Q, K)\n",
        "        R = self.softmax(qk*self.dscale)\n",
        "        R = torch.einsum(\"...ic,...cn->...in\", R, V) # Scaled Dot-Product Attention\n",
        "        O = self.Wr(R) # [..., C, N] * [N, pq] -> [..., C, pq]\n",
        "        O = O + image\n",
        "        O = self.LN(O)\n",
        "        if ret_attn_QKV:\n",
        "            return O, Q, K, V\n",
        "        return O\n",
        "\n",
        "class PatchImage(nn.Module):\n",
        "    def __init__(self, patch_size, reverse=False):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.n = int(self.patch_size**(0.5))\n",
        "        assert self.n**2 == patch_size, \"patch_size must be full square\"\n",
        "        self.reverse = reverse\n",
        "    def forward(self, x):\n",
        "\n",
        "        n = self.n\n",
        "        if self.reverse:\n",
        "            b, c, h, w, s = x.shape\n",
        "            x = torch.reshape(x, (b, c, h, w, n, n))\n",
        "            x = torch.transpose(x, -2, -3)\n",
        "            x = torch.reshape(x, (b, c, h*n, w*n))\n",
        "            return x\n",
        "        b, c, h, w = x.shape\n",
        "        x = torch.reshape(x, (b, c, h//n, n, w//n, n))\n",
        "        x = torch.transpose(x, -2, -3)\n",
        "        x = torch.reshape(x, (b, c, h//n, w//n, n*n))\n",
        "        return x\n",
        "        #torch.reshape(torch.transpose(torch.reshape(a, (b, c, h//n, n, w//n, n)), -2, -3), (b, c, h//n, w//n, n*n))"
      ]
    }
  ]
}