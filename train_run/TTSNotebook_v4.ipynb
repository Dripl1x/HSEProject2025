{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "QCH5nYQmv1cA"
   },
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.cuda.amp import autocast\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from torch import amp\n",
    "from torch.amp import GradScaler\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, RandomSampler\n",
    "from torch.optim.swa_utils import AveragedModel    \n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn.functional import pad\n",
    "\n",
    "import torchvision.utils as vutils\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b20J1mRIGR3-",
    "outputId": "1b8b0305-f163-415e-ed4f-e1c41b1c094d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "T6EsppzSwIKN"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"dataset\": {\n",
    "        \"train\": {\n",
    "            \"table\": \"E:/data/train.csv\",\n",
    "            \"data\": \"E:/data/bare_data/\"\n",
    "        },\n",
    "        \"val\": {\n",
    "            \"table\": \"E:/data/val.csv\",\n",
    "            \"data\": \"E:/data/bare_data/\"\n",
    "        }\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"batch_size\": 16,\n",
    "        \"grad_acum\": 1,\n",
    "        \"dtype\": \"float32\",\n",
    "        'shuffle': True,\n",
    "        'pin_memory': True,\n",
    "    },\n",
    "    \"val\": {\n",
    "        \"batch_size\": 16,\n",
    "        \"grad_acum\": 1,\n",
    "        \"dtype\": \"float32\",\n",
    "        'shuffle': False,\n",
    "        'pin_memory': True,\n",
    "    },\n",
    "    \"vae\": {\n",
    "        \"freq\": 16000,\n",
    "        \"lenght\": 5,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"latent_size\": 128,\n",
    "        \"epochs\": 15,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"freq_scale\": 4,\n",
    "        \"time_scale\": 4,\n",
    "    },\n",
    "    \"utils\": {\n",
    "        \"n_fft\": 800, # TODO\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58r16JvWGR4B"
   },
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "tU0zMRsTGR4B"
   },
   "outputs": [],
   "source": [
    "class VAE_Audio(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        self.encoder_input = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 1, 1), nn.BatchNorm2d(32), nn.GELU(),\n",
    "            nn.Conv2d(32, 16, 3, 1, 1), nn.BatchNorm2d(16), nn.GELU(),\n",
    "        )\n",
    "        self.encoder_squeeze = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, 3, 2, 1), nn.GELU(),\n",
    "            nn.Conv2d(16, 16, 3, 2, 1), nn.GELU(),\n",
    "        )\n",
    "        self.encoder_mu     = nn.Conv2d(16, 32, 1)\n",
    "        self.encoder_logvar = nn.Conv2d(16, 32, 1)\n",
    "        self.decoder_unsqueeze = nn.Sequential(\n",
    "                nn.ConvTranspose2d(32, 32, 3, 2, 1, output_padding=1), nn.GELU(),\n",
    "                nn.ConvTranspose2d(32, 16, 3, 2, 1, output_padding=1), nn.GELU(),\n",
    "        )\n",
    "        self.decoder_output = nn.Sequential(\n",
    "                 nn.ConvTranspose2d(16, 4, 3, 1, 1), nn.GELU(),\n",
    "                 nn.BatchNorm2d(4),\n",
    "                 nn.ConvTranspose2d(4, 1, 3, 1, 1),\n",
    "        )\n",
    "    def encode(self, x):\n",
    "        x = self.encoder_input(x)\n",
    "        x = self.encoder_squeeze(x)\n",
    "        mu = self.encoder_mu(x)\n",
    "        logvar = self.encoder_logvar(x)\n",
    "        return mu, logvar\n",
    "    def sample(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        z = mu + std * torch.randn_like(std)\n",
    "        return z, mu, logvar\n",
    "    def decode(self, x):\n",
    "        x = self.decoder_unsqueeze(x)\n",
    "        x = self.decoder_output(x)\n",
    "        return x\n",
    "    def KLD_loss(self, mu, logvar, q=0.02):\n",
    "        kld = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        kld = torch.clamp(kld, min=q)\n",
    "        return kld.mean()\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.sample(x)\n",
    "        return self.decode(z), z, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaLlnVbzwL9g"
   },
   "source": [
    "# Моделька"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "sJF9tFrIdzzT"
   },
   "outputs": [],
   "source": [
    "class NoiseScheduler:\n",
    "    def __init__(self, timestamps=100, epochs=100):\n",
    "        self.steps = timestamps\n",
    "        self.epochs = epochs\n",
    "        self.betas = torch.linspace(0.02, 0.0004, timestamps)\n",
    "        self.alpha = 1 - self.betas\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        return self.betas[step]\n",
    "    def corrupt_image(self, image, idx):\n",
    "        noise = torch.randn_like(image)\n",
    "        b = image.shape[0]\n",
    "        k = self.alpha_hat[idx].view(b, 1, 1, 1)\n",
    "        return torch.sqrt(k)*image + torch.sqrt(1-k)*noise, noise\n",
    "\n",
    "    def sample_timestamps(self, iters=10):\n",
    "        indicies = torch.cat((torch.randint(0, self.steps-1, [iters-1]), torch.tensor(100)))\n",
    "        return indicies\n",
    "\n",
    "    def restore_image(self, image, pred, idx, sigma=0.0):\n",
    "        noise = torch.randn_like(image)\n",
    "        alpha = self.alpha[idx]\n",
    "        alpha_hat = self.alpha_hat[idx]\n",
    "        nalpha = 1-alpha\n",
    "        nalpha_hat = 1-alpha_hat\n",
    "        return (image - pred*nalpha/(torch.sqrt(nalpha_hat)))/torch.sqrt(alpha) + sigma*noise\n",
    "\n",
    "    def get_idx(self, epoch, batch_size):\n",
    "        return self.steps-torch.randint(0, self.steps, [batch_size]) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "raT7SjBAMMIY"
   },
   "outputs": [],
   "source": [
    "class ResudialBlock(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(*args)\n",
    "    def forward(self, x):\n",
    "        return x + self.layers(x)\n",
    "\n",
    "class ConditionMixingLayer(nn.Module):\n",
    "    def __init__(self, input_channels, conditioning_length, hidden_size = 8):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cond_proj = nn.Linear(conditioning_length, self.hidden_size)\n",
    "        self.conv_proj = nn.Conv2d(input_channels, self.hidden_size, 3, 1, 1)\n",
    "        self.lin_proj = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.lin1_unproj = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.lin2_unproj = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.conv_unproj = nn.Conv2d(self.hidden_size, input_channels, 3, 1, 1)\n",
    "        self.conv_act = nn.Tanh()\n",
    "        self.bn1 = nn.BatchNorm2d(input_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(input_channels)\n",
    "\n",
    "\n",
    "        self.add_a = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.add_b = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "    def forward(self, x, c=None, skip=False):\n",
    "        if len(x.shape)==3:\n",
    "            x = x.unsqueeze(0)\n",
    "        b, ch, h, w = x.shape\n",
    "        x = self.bn1(x)\n",
    "        xn = self.conv_proj(x) # [B, N, H, W]\n",
    "        xn = self.conv_act(xn)\n",
    "        xn = xn.view(b, h*w, self.hidden_size) # [B, H*W, N]\n",
    "        xn = self.lin_proj(xn) # [B, H*W, N]\n",
    "        xn = self.conv_act(xn) # [B, H*W, N]\n",
    "        if not skip:\n",
    "            cn = self.cond_proj(c) # [B, N]\n",
    "            cn = self.conv_act(cn) # [B, N]\n",
    "            cn = cn.view(b, 1, self.hidden_size)\n",
    "            xn = self.add_a(xn) # [B, 1, N]\n",
    "            cn = self.add_b(cn) # [B, H*W, N]\n",
    "            xn = xn + cn # [B, H*W, N]\n",
    "\n",
    "        xn = self.lin1_unproj(xn) # [B, H*W, N]\n",
    "        xn = self.conv_act(xn) # [B, H*W, N]\n",
    "        xn = self.lin2_unproj(xn) # [B, H*W, N]\n",
    "        xn = self.conv_act(xn) # [B, H*W, N]\n",
    "\n",
    "        xn = xn.view(b, self.hidden_size, h, w) # [B, N, H, W]\n",
    "        xn = self.conv_unproj(xn)\n",
    "        xn = self.conv_act(xn) # [B, I, H, W]\n",
    "        xn = self.bn2(xn)\n",
    "\n",
    "        x = xn + x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "Vtu2x49DMU5R"
   },
   "outputs": [],
   "source": [
    "class OurAttentionLayer(nn.Module):\n",
    "    def __init__(self, patch_size, channels_in, hidden_dim, emb_size=1, cross=False):\n",
    "        super().__init__()\n",
    "        self.cross = cross\n",
    "        self.hidden_dim = hidden_dim #N\n",
    "        self.channels_in = channels_in #C\n",
    "        self.patch_size = patch_size #pq\n",
    "        self.emb_size = emb_size #Z\n",
    "        self.Wk = nn.Linear(patch_size, hidden_dim)     # [pq, N]\n",
    "        self.Wv = nn.Linear(patch_size, hidden_dim)     # [pq, N]\n",
    "        self.LN = nn.LayerNorm([channels_in, patch_size])\n",
    "        if cross:\n",
    "            self.Wi = nn.Linear(emb_size, channels_in)  # [Z, C]\n",
    "            self.Wj = nn.Linear(emb_size, hidden_dim)   # [Z, N]\n",
    "            self.Wq = nn.Linear(hidden_dim, hidden_dim) # [N, N]\n",
    "        else:\n",
    "            self.Wq = nn.Linear(patch_size, hidden_dim) # [C, N]\n",
    "        self.Wr = nn.Linear(hidden_dim, patch_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dscale = 1/(hidden_dim**0.5)\n",
    "    def forward(self, image, text = None, ret_attn_QKV=False):\n",
    "        # image == [Batch, channels, patch_size] == [..., C, pq]\n",
    "        K = self.Wk(image) # [..., C, pq] * [pq, N] = [..., C, N]\n",
    "        V = self.Wv(image)\n",
    "        if self.cross and text is None:\n",
    "            text = torch.rand(1, self.emb_size)\n",
    "        if self.cross:\n",
    "            #text_T = torch.permute(text, (-1, -2))\n",
    "            # text = [Batch, seq_len, emb_size] == [..., S, Z]\n",
    "            I = self.Wi(text) # [..., S, Z] * [Z, C] -> [..., S, C]\n",
    "            J = self.Wj(text) # [..., S, Z] * [Z, N] -> [..., S, N]\n",
    "            Q1 = torch.einsum(\"...sc,...sn->...cn\", I, J) # возможно надо отдебажить учитывая Batch и прочее\n",
    "            # [..., C, S] * [..., S, N] -> C, N\n",
    "            Q = self.Wq(Q1).unsqueeze(1).expand_as(K) # -> C, N\n",
    "        else:\n",
    "            Q = self.Wq(image) # [..., C, pq] * [pq, N] = [..., C, N]\n",
    "\n",
    "        qk = torch.einsum(\"...jn,...cn->...cj\", Q, K)\n",
    "        R = self.softmax(qk*self.dscale)\n",
    "        R = torch.einsum(\"...ic,...cn->...in\", R, V) # Scaled Dot-Product Attention\n",
    "        O = self.Wr(R) # [..., C, N] * [N, pq] -> [..., C, pq]\n",
    "        O = O + image\n",
    "        O = self.LN(O)\n",
    "        if ret_attn_QKV:\n",
    "            return O, Q, K, V\n",
    "        return O\n",
    "\n",
    "class PatchImage(nn.Module):\n",
    "    def __init__(self, patch_size, reverse=False):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.n = int(self.patch_size**(0.5))\n",
    "        assert self.n**2 == patch_size, \"patch_size must be full square\"\n",
    "        self.reverse = reverse\n",
    "    def forward(self, x):\n",
    "\n",
    "        n = self.n\n",
    "        if self.reverse:\n",
    "            b, c, h, w, s = x.shape\n",
    "            x = torch.reshape(x, (b, c, h, w, n, n))\n",
    "            x = torch.transpose(x, -2, -3)\n",
    "            x = torch.reshape(x, (b, c, h*n, w*n))\n",
    "            return x\n",
    "        b, c, h, w = x.shape\n",
    "        x = torch.reshape(x, (b, c, h//n, n, w//n, n))\n",
    "        x = torch.transpose(x, -2, -3)\n",
    "        x = torch.reshape(x, (b, c, h//n, w//n, n*n))\n",
    "        return x\n",
    "        #torch.reshape(torch.transpose(torch.reshape(a, (b, c, h//n, n, w//n, n)), -2, -3), (b, c, h//n, w//n, n*n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_diffusion(nn.Module):\n",
    "    def __init__(self, input_channels=1, conditioning_length=1, timestamp_length=1, hidden_dims=32, mixin_dims=32):\n",
    "        super().__init__()\n",
    "        self.input_scaler = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=input_channels, out_channels=hidden_dims, kernel_size=1), nn.Tanh()\n",
    "            )\n",
    "        self.output_scaler = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=hidden_dims, out_channels=hidden_dims, kernel_size=1), nn.Tanh(),\n",
    "                nn.Conv2d(in_channels=hidden_dims, out_channels=input_channels, kernel_size=1)\n",
    "            )\n",
    "        self.precode = nn.Sequential(\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.Tanh(), nn.BatchNorm2d(hidden_dims),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.Tanh(), nn.BatchNorm2d(hidden_dims),\n",
    "            )\n",
    "        self.mixing_layer = ConditionMixingLayer(hidden_dims, conditioning_length+timestamp_length, mixin_dims)\n",
    "        self.encoder = nn.Sequential(\n",
    "            ResudialBlock(\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.Tanh(), nn.BatchNorm2d(hidden_dims),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.Tanh(), nn.BatchNorm2d(hidden_dims),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.Tanh(), nn.BatchNorm2d(hidden_dims),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(hidden_dims, hidden_dims*4, 1), nn.Tanh(), nn.LazyInstanceNorm2d(),\n",
    "                nn.PixelShuffle(2), #C/4\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 1), nn.Tanh(), nn.LazyInstanceNorm2d(),\n",
    "                nn.PixelUnshuffle(2), #C*4\n",
    "                nn.Conv2d(hidden_dims*4, hidden_dims, 1), nn.Tanh(), nn.LazyInstanceNorm2d(),\n",
    "            ),\n",
    "            ResudialBlock(\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.Tanh(), nn.LazyInstanceNorm2d(),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.Tanh(), nn.LazyInstanceNorm2d(),\n",
    "            ),\n",
    "        )\n",
    "        self.downscaler = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dims, hidden_dims, 3, 2, 1), nn.BatchNorm2d(hidden_dims), nn.Tanh(),\n",
    "            nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.Tanh(), nn.BatchNorm2d(hidden_dims),\n",
    "            nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.Tanh(), nn.BatchNorm2d(hidden_dims),\n",
    "        )\n",
    "\n",
    "        self.upscaler = nn.Sequential(\n",
    "            nn.ConvTranspose2d(hidden_dims, hidden_dims, 3, 2, 1, output_padding=1), nn.BatchNorm2d(hidden_dims), nn.Tanh(),\n",
    "            nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.Tanh(), nn.BatchNorm2d(hidden_dims),\n",
    "            nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.Tanh(), nn.BatchNorm2d(hidden_dims),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            ResudialBlock(\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.Tanh(), nn.BatchNorm2d(hidden_dims),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.Tanh(), nn.BatchNorm2d(hidden_dims),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(hidden_dims, hidden_dims*4, 1), nn.Tanh(), nn.LazyInstanceNorm2d(),\n",
    "                nn.PixelShuffle(2), #C/4\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 1), nn.Tanh(), nn.LazyInstanceNorm2d(),\n",
    "                nn.PixelUnshuffle(2), #C*4\n",
    "                nn.Conv2d(hidden_dims*4, hidden_dims, 1), nn.Tanh(), nn.LazyInstanceNorm2d(),\n",
    "            ),\n",
    "            ResudialBlock(\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.Tanh(), nn.BatchNorm2d(hidden_dims),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.Tanh(), nn.BatchNorm2d(hidden_dims),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.Tanh(), nn.BatchNorm2d(hidden_dims),\n",
    "            ),\n",
    "        )\n",
    "        self.padder = nn.ZeroPad2d(1)\n",
    "    def encode(self, x, mix = False, c = None):\n",
    "        x = self.input_scaler(x)\n",
    "        x = self.precode(x)\n",
    "        if mix and not c is None:\n",
    "            x = self.mixing_layer(x, c, False)\n",
    "        else:\n",
    "            x = self.mixing_layer(x, c, True)\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "    def rescale(self, x, n=0):\n",
    "        if n<=0:\n",
    "            return x\n",
    "        b, c, ho, wo = x.shape\n",
    "        if not (ho%2==0 and wo%2==0):\n",
    "            y = self.padder(x)\n",
    "        else:\n",
    "            y = x\n",
    "        y = self.downscaler(y)\n",
    "        b, c, h, w = y.shape\n",
    "        z = self.rescale(y, n-1)\n",
    "        z = nn.functional.interpolate(z, [h, w])\n",
    "        y = y + z\n",
    "        y = self.upscaler(y)\n",
    "        y = nn.functional.interpolate(y, [ho, wo])\n",
    "        x = y + x\n",
    "        return x\n",
    "    def decode(self, x):\n",
    "        x = self.decoder(x)\n",
    "        x = self.output_scaler(x)\n",
    "        return x\n",
    "    def forward(self, x, mix=False, c = None, n=1):\n",
    "        x = self.encode(x, mix, c)\n",
    "        x = self.rescale(x, n)\n",
    "        x = self.decode(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "bwz9Ju6DkdX2"
   },
   "outputs": [],
   "source": [
    "class Word_Encoder(nn.Module): # Токенайзер + Эмбеддер для букв\n",
    "    def __init__(self, alphabet, emb_size, max_word_size = 256):\n",
    "        super().__init__()\n",
    "        self.alphabet = list(alphabet)+[\"<pad>\", \"<stress>\", \"<unk>\"] # буквы + спец токены: пустой, ударение и неизвестный символ\n",
    "        self.emb_size = emb_size\n",
    "        self.embeddings = nn.Embedding(len(self.alphabet), emb_size)\n",
    "        self.pos_embeddings = nn.Embedding(max_word_size, emb_size)\n",
    "        self.device = self.embeddings.device\n",
    "\n",
    "class Word_Encoder(nn.Module):\n",
    "    def __init__(self, alphabet, emb_size, max_word_size=256):\n",
    "        super().__init__()\n",
    "        self.alphabet = list(alphabet) + [\"<pad>\", \"<stress>\", \"<unk>\"]\n",
    "        self.emb_size = emb_size\n",
    "        self.max_word_size = max_word_size\n",
    "        self.embeddings = nn.Embedding(len(self.alphabet), emb_size)\n",
    "        self.pos_embeddings = nn.Embedding(max_word_size, emb_size)\n",
    "\n",
    "        self.get_idx = {char: idx for idx, char in enumerate(self.alphabet)}\n",
    "        self.pad_idx = self.get_idx[\"<pad>\"]\n",
    "        self.stress_idx = self.get_idx[\"<stress>\"]\n",
    "        self.unk_idx = self.get_idx[\"<unk>\"]\n",
    "        self.device = self.embeddings.weight.device\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        if isinstance(text, str):\n",
    "            text = [text]\n",
    "        tokenized = []\n",
    "        for word in text:\n",
    "            word_idxs = []\n",
    "            i = 0\n",
    "            n = len(word)\n",
    "            while i < n:\n",
    "                if word[i] == \"<\" and i + 8 < n and word[i:i+8] == \"<stress>\":\n",
    "                  word_idxs.append(self.stress_idx)\n",
    "                  i += 8\n",
    "                else:\n",
    "                    char = word[i]\n",
    "                    if char in self.get_idx:\n",
    "                        word_idxs.append(self.get_idx[char])\n",
    "                    else:\n",
    "                        word_idxs.append(self.unk_idx)\n",
    "                    i += 1\n",
    "\n",
    "            tokenized.append(word_idxs)\n",
    "        max_len = max(len(word) for word in tokenized)\n",
    "        padded = []\n",
    "        for word in tokenized:\n",
    "            padded_word = word\n",
    "            if len(word) < max_len:\n",
    "                padded_word += [self.pad_idx] * (max_len - len(word))\n",
    "            padded.append(padded_word)\n",
    "\n",
    "        return torch.tensor(padded, dtype=torch.long, device=self.device)\n",
    "\n",
    "    def forward(self, x): # Не забыть проверить работу с батчами\n",
    "        self.device = x.device\n",
    "        batch, n = x.shape\n",
    "        pos = torch.arange(n, device=self.device).unsquezze(0).expand(batch, n)\n",
    "        x = self.embeddings(x) + self.pos_embeddings(pos)\n",
    "        return x\n",
    "\n",
    "class Noise_Encoder(nn.Module):\n",
    "    def __init__(self, emb_size, timestamps = 1000):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(timestamps, emb_size)\n",
    "    def forward(self, x): # ✓ Не забыть проверить работу с батчами\n",
    "        self.device = x.device\n",
    "        return self.embeddings(x)\n",
    "class Time_Encoder(nn.Module): # ✓ до 10 секунд\n",
    "    def __init__(self, in_channels, out_channels, max_time_size=1024,): # Посмотреть максимальный размер по x, поставить на 20-50% больше\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, (3, 7), (1, 3), (1, 3))\n",
    "        self.pos_embs =  nn.Embedding(max_time_size, out_channels)\n",
    "        self.max_time_size = max_time_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, (3, 7), (1, 3), (1, 3))\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels, (3, 7), (1, 3), (1, 3))\n",
    "        self.act = nn.Tanh()\n",
    "    def forward(self, image):\n",
    "        x = self.conv1(image)\n",
    "        b, c, h, w = x.shape\n",
    "        y = self.conv2(image)\n",
    "        time = torch.arange(w).expand([b, w])\n",
    "        pos = self.pos_embs(time) # [b, w, out]\n",
    "        pos = torch.permute(pos, [0, 2, 1]).unsqueeze(1) # b, 1, c, w\n",
    "        x = torch.permute(x, [0, 2, 1, 3]) # b, h, c, w\n",
    "        pos = pos.expand_as(x)\n",
    "        x = x + pos # [b, h, c, w]+[b, h, c, w]\n",
    "        y = self.act(y)\n",
    "        x = torch.permute(x, [0, 2, 1, 3])\n",
    "        z = self.conv3(x)+y\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "giy9WAb2wOTx"
   },
   "outputs": [],
   "source": [
    "class TTS_diffusion(nn.Module):\n",
    "    def __init__(self, input_channels = 1, hidden_dims = 32, alphabet = \"абвгдеёжзийклмнопрстуфхцчшщъыьэюя\", emb_size_word = 128, emb_size_noise = 64,\n",
    "                       noise_steps = 100, max_word_size = 256, max_time_size = 2048):\n",
    "        super().__init__()\n",
    "        self.input_scaler = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=input_channels, out_channels=hidden_dims, kernel_size=1), nn.Tanh()\n",
    "            )\n",
    "        self.precode = nn.Sequential(\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.Tanh(), nn.BatchNorm2d(hidden_dims),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 2, 1), nn.BatchNorm2d(hidden_dims), nn.Tanh(),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 2, 1), nn.BatchNorm2d(hidden_dims), nn.Tanh(),\n",
    "                nn.ConvTranspose2d(hidden_dims, hidden_dims, 3, 2, 1, output_padding=1), nn.BatchNorm2d(hidden_dims), nn.Tanh(),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.Tanh(), nn.BatchNorm2d(hidden_dims),\n",
    "                nn.ConvTranspose2d(hidden_dims, hidden_dims, 3, 2, 1, output_padding=1), nn.BatchNorm2d(hidden_dims), nn.Tanh(),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.Tanh(), nn.BatchNorm2d(hidden_dims),\n",
    "            ) # ✓ что бы сделать размер кратным 4\n",
    "        self.time_enc = Time_Encoder(hidden_dims, hidden_dims, max_time_size) # ✓ args сюда нужно пихнуть число каналов после precode + \n",
    "                                                                              # ✓ сколько хотим вернуть (лучше чуть больше) + максимальный размер картинки по X после MelSpec или как там оно\n",
    "        self.post_time =  nn.Sequential(\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.Tanh(), nn.BatchNorm2d(hidden_dims),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.Tanh(), nn.BatchNorm2d(hidden_dims),\n",
    "        )\n",
    "        self.word_enc = Word_Encoder(alphabet, emb_size_word, max_word_size) # ✓ args сюда нужно пихнуть алфавит, размер эмбеда (emb_size_word) + длину слова\n",
    "\n",
    "        self.noise_enc = Noise_Encoder(emb_size_noise, noise_steps) # ✓ Пихнуть сюда число шагов в нойз шедулере и размер эмбеда (emb_size_noise)\n",
    "        self.patch_img1_size = 16\n",
    "        self.patch_img1 = PatchImage(self.patch_img1_size) # ✓ можно пробовать другие размеры, лучше больше 4\n",
    "        self.atten_noise1 = OurAttentionLayer(self.patch_img1_size, hidden_dims, hidden_dims, emb_size_noise, True)\n",
    "        self.atten_word1 = OurAttentionLayer(self.patch_img1_size, hidden_dims, hidden_dims, emb_size_word, True)\n",
    "        self.atten_word2 = OurAttentionLayer(self.patch_img1_size, hidden_dims, hidden_dims, emb_size_word, True)\n",
    "        self.satten1 = OurAttentionLayer(self.patch_img1_size, hidden_dims, hidden_dims, 1, False)\n",
    "        self.unpatch_img1 = PatchImage(self.patch_img1_size, True)\n",
    "        self.main_block1 = nn.Sequential(\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.Tanh(), nn.BatchNorm2d(hidden_dims),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 2, 1), nn.BatchNorm2d(hidden_dims), nn.Tanh(),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 2, 1), nn.BatchNorm2d(hidden_dims), nn.Tanh(),\n",
    "            ) # ✓ сжимает в 4 раза\n",
    "        \n",
    "        # ✓ перед применением при помощи interpolate сделать картинку кратной sqrt(patch_size) по H и W (или использовать shuffle layers + конвы)\n",
    "        self.patch_img2_size = 16\n",
    "        self.patch_img2 = PatchImage(self.patch_img2_size) # ✓ можно пробовать другие размеры\n",
    "        self.atten_word3 = OurAttentionLayer(self.patch_img2_size, hidden_dims, hidden_dims, emb_size_word, True)\n",
    "        self.satten2 = OurAttentionLayer(self.patch_img2_size, hidden_dims, hidden_dims, 1, False)\n",
    "        self.atten_word4 = OurAttentionLayer(self.patch_img2_size, hidden_dims, hidden_dims, emb_size_word, True)\n",
    "        self.unpatch_img2 = PatchImage(self.patch_img2_size, True)\n",
    "        self.main_block2 = nn.Sequential(\n",
    "                nn.ConvTranspose2d(hidden_dims, hidden_dims, 3, 2, 1, output_padding=1), nn.BatchNorm2d(hidden_dims), nn.Tanh(),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.Tanh(), nn.BatchNorm2d(hidden_dims),\n",
    "                nn.ConvTranspose2d(hidden_dims, hidden_dims, 3, 2, 1, output_padding=1), nn.BatchNorm2d(hidden_dims), nn.Tanh(),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.Tanh(), nn.BatchNorm2d(hidden_dims),\n",
    "        )\n",
    "        self.patch_img3_size = 16\n",
    "        self.patch_img3 = PatchImage(self.patch_img3_size) # ✓ можно пробовать другие размеры\n",
    "        self.atten_noise2 = OurAttentionLayer(self.patch_img3_size, hidden_dims, hidden_dims, emb_size_noise, True)\n",
    "        self.atten_word5 = OurAttentionLayer(self.patch_img3_size, hidden_dims, hidden_dims, emb_size_word, True)\n",
    "        self.atten_word6 = OurAttentionLayer(self.patch_img3_size, hidden_dims, hidden_dims, emb_size_word, True)\n",
    "        self.satten3 = OurAttentionLayer(self.patch_img3_size, hidden_dims, hidden_dims, 1, False)\n",
    "        self.unpatch_img3 = PatchImage(self.patch_img3_size, True)\n",
    "        self.main_block3 = nn.Sequential(\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.Tanh(), nn.BatchNorm2d(hidden_dims),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.Tanh(), nn.BatchNorm2d(hidden_dims),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.Tanh(), nn.BatchNorm2d(hidden_dims),\n",
    "        )\n",
    "        self.output_scaler = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=hidden_dims, out_channels=input_channels, kernel_size=1), nn.Tanh(),\n",
    "        )\n",
    "        self.out_scaler_conv1 = nn.Conv2d(in_channels=hidden_dims, out_channels=input_channels, kernel_size=1)\n",
    "        self.out_scaler_conv2 = nn.Conv2d(in_channels=input_channels, out_channels=input_channels, kernel_size=1)\n",
    "    def drop(self, layer, x, *args):\n",
    "        if self.training and torch.rand(1).item() < 0.1:\n",
    "            return x\n",
    "        return layer(x, *args)\n",
    "    def forward(self, x, text, noise): # ✓ в трейне иногда дропать каждый (делать torch.rand) с вероятностью 1/10\n",
    "        x = self.input_scaler(x)\n",
    "        words = self.word_enc(text)\n",
    "        sh = self.noise_enc(noise)\n",
    "        x = self.time_enc(x)\n",
    "        x = self.post_time(x)\n",
    "        x = self.precode(x)\n",
    "        \n",
    "        x = self.resize_to_square(x, self.patch_img1_size)\n",
    "        x = self.patch_img1(x)\n",
    "        x = self.drop(self.atten_noise1, x, sh) # ✓ p = 0.1\n",
    "        x = self.atten_word1(x, words)\n",
    "        x = self.atten_word2(x, words)\n",
    "        x = self.satten1(x)\n",
    "        x = self.unpatch_img1(x)\n",
    "        x = self.main_block1(x)\n",
    "\n",
    "        x = self.resize_to_square(x, self.patch_img2_size)\n",
    "        x = self.patch_img2(x)\n",
    "        x = self.atten_word3(x, words)\n",
    "        x = self.atten_word4(x, words)\n",
    "        x = self.satten2(x)\n",
    "        x = self.unpatch_img2(x)\n",
    "        x = self.main_block2(x)\n",
    "        # ✓ как-то напихать все что есть\n",
    "        x = self.resize_to_square(x, self.patch_img3_size)\n",
    "        x = self.patch_img3(x)\n",
    "        x = self.drop(self.atten_noise2, x, sh) # ✓ p = 0.1\n",
    "        x = self.atten_word5(x, words)\n",
    "        x = self.atten_word6(x, words)\n",
    "        x = self.satten3(x)\n",
    "        x = self.unpatch_img3(x)\n",
    "        x = self.main_block3(x)\n",
    "        \n",
    "        y = self.out_scaler_conv1(x)\n",
    "        x = self.output_scaler(x)\n",
    "        x = x * y\n",
    "        x = self.out_scaler_conv2(x)\n",
    "        return x\n",
    "    def resize_to_square(self, x, patch_size):\n",
    "        n = int(math.sqrt(patch_size))\n",
    "        h1, w1 = x.shape[-2:]\n",
    "        h2, w2 = ((h1 + n - 1) // n) * n, ((w1 + n - 1) // n) * n\n",
    "        if h2 != h1 or w2 != w1:\n",
    "            x = torch.nn.functional.interpolate(x, size=(h2, w2), mode=\"bilinear\", align_corners=False)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45dnwlxUwSRg"
   },
   "source": [
    "# Тренер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "BNu1TxO0wTvp"
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "\n",
    "class AvegereMeter:\n",
    "    def __init__(self,):\n",
    "        self.arr = []\n",
    "    def __call__(self, item, n=1):\n",
    "        if n<=1:\n",
    "            self.arr.extend([item])\n",
    "        else:\n",
    "            self.arr.extend([item]*n)\n",
    "    def __str__(self,) -> str:\n",
    "        return str(np.mean(np.array(self.arr)))\n",
    "    def zero(self,):\n",
    "        self.arr=[]\n",
    "\n",
    "class TTS_Trainer:\n",
    "    def __init__(self, model, train_dataloader, val_dataloader, vae, epochs=10, ):\n",
    "        self.model = model\n",
    "        self.vae = vae\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.tdl = train_dataloader\n",
    "        self.vdl = val_dataloader\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=5e-4, weight_decay=1e-2)\n",
    "        self.scaler = GradScaler(device=\"cuda\")\n",
    "        self.l2_loss = nn.MSELoss(reduction='none')\n",
    "        self.loss_meter = AvegereMeter()\n",
    "        self.epochs = epochs\n",
    "        self.losses = []\n",
    "        self.noise_sched = NoiseScheduler(100, epochs)\n",
    "        self.noise_sched.alpha = self.noise_sched.alpha.to(self.device)\n",
    "        self.noise_sched.alpha_hat = self.noise_sched.alpha_hat.to(self.device)\n",
    "\n",
    "    def draw_diffusion(self, S=5, epoch=0):\n",
    "        fig, axes = plt.subplots(11, S+1, figsize=(S+1, 11))\n",
    "        noise = torch.randn([1, 32, 7, 7], device=self.device)\n",
    "        transformed_weight = [noise.clone() for _ in range(11)]\n",
    "        for i in range(0, 11):\n",
    "            for j in range(S+1):\n",
    "\n",
    "                if j>0:\n",
    "                    #noise[0][0].numpy()+\n",
    "                    idx = int((j/S)*99)\n",
    "                    k = self.noise_sched(idx).item()\n",
    "                    pred = self.model(transformed_weight[i], i<10, torch.tensor([*[0]*i,1,*[0]*(9-i), idx], dtype=torch.float32, device=self.device))\n",
    "                    transformed_weight[i] = self.noise_sched.restore_image(transformed_weight[i], pred, idx, 1e-5)\n",
    "\n",
    "                decoded = self.vae.decode(transformed_weight[i])\n",
    "                img = decoded.squeeze(0).squeeze(0).cpu().detach().numpy()\n",
    "                axes[i, j].imshow(img)\n",
    "                axes[i, j].axis('off')\n",
    "                if i==0:\n",
    "                    axes[i, j].set_title(f'{j/S}')\n",
    "        os.makedirs('./train/dl1/', exist_ok=True)\n",
    "        plt.savefig(f'./train/dl1/diff{S}_{epoch}.png')\n",
    "        plt.close(fig)\n",
    "\n",
    "    def train_loop(self, KLDk=0.05, epoch=0):\n",
    "        self.model.train()\n",
    "        self.loss_meter.zero()\n",
    "        pbar = tqdm(self.tdl, desc = 'train')\n",
    "        cnt = 0\n",
    "        self.vae.train()\n",
    "        for mel, _ in pbar:\n",
    "            mel = mel.to(self.device).float()\n",
    "            b, c, h, w = mel.shape\n",
    "            _, mu, logvar = self.vae.sample(mel)   # mu, logvar — [B,16,H',W']\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            z = mu + eps * std \n",
    "            #mel = z\n",
    "            with torch.autocast('cuda', dtype=torch.bfloat16):\n",
    "                idx = self.noise_sched.get_idx(epoch, b).to(self.device)\n",
    "                model_input, noise = self.noise_sched.corrupt_image(z, idx)\n",
    "                model_input = model_input.to(self.device)\n",
    "                noise = noise.to(self.device)\n",
    "                text_ids = torch.zeros(b, 1, dtype=torch.long, device=self.device)\n",
    "                cond_onehot = nn.functional.one_hot(cond, num_classes=10).float().to(self.device)\n",
    "                condition = torch.cat([cond_onehot, idx.view(-1,1)], dim=1)\n",
    "\n",
    "                output = self.model(model_input, text_ids, idx)\n",
    "        \n",
    "                noise_loss = self.l2_loss(output, noise).mean()\n",
    "                kld_loss = self.vae.KLD_loss(mu, logvar)\n",
    "                loss = noise_loss + KLDk * kld_loss\n",
    "        \n",
    "            self.optimizer.zero_grad()\n",
    "            self.losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            cur_loss = loss.detach().cpu().item()\n",
    "            pbar.desc = f\"train, loss={cur_loss:.3f}\"\n",
    "            self.loss_meter(cur_loss, mel.size(0))\n",
    "        print(\"Loss = \"+self.loss_meter.__str__())\n",
    "\n",
    "    def save_loss(self, filepath):\n",
    "        with open(filepath, \"w\") as f:\n",
    "            f.write(str(self.loss_meter))\n",
    "\n",
    "    def save_image(self, audio, output, iter = 0):\n",
    "        input_tensor = audio.cpu().detach().squeeze(0)   # shape [1, H, W]\n",
    "        output_tensor = output.cpu().detach().squeeze(0)  # shape [1, H, W]\n",
    "\n",
    "        # Преобразование в PIL-изображение\n",
    "        transform = transforms.ToPILImage('RGB')\n",
    "        rgb_input = torch.cat([input_tensor,  input_tensor,  input_tensor ], dim=0)\n",
    "        rgb_output = torch.cat([output_tensor, output_tensor, output_tensor], dim=0)\n",
    "        input_image = transform(rgb_input)\n",
    "        output_image = transform(rgb_output)\n",
    "\n",
    "        os.makedirs('./train', exist_ok=True)\n",
    "        # Сохранение изображений\n",
    "        input_image.save(f'./train/input_{iter}.png')\n",
    "        output_image.save(f'./train/output_{iter}.png')\n",
    "\n",
    "    def val_loop(self, iteration=0):\n",
    "        self.model.eval()\n",
    "        batch = next(iter(self.vdl))\n",
    "        if isinstance(batch, (list, tuple)):\n",
    "            mel = batch[0]                  \n",
    "        else:\n",
    "            mel = batch\n",
    "        mel = mel.to(self.device, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            z0 = self.vae.sample(mel[:1])[0]            # [1,C,H,W]\n",
    "        cond_onehot = torch.zeros(1, 10, device=self.device)  # [1,10]\n",
    "        t_start = torch.tensor([[float(self.noise_sched.steps-1)]], device=self.device)\n",
    "        condition  = torch.cat([cond_onehot, t_start], dim=1)  # [1,11]\n",
    "        v = torch.randn_like(z0)        # x_T\n",
    "        T = self.noise_sched.steps\n",
    "        snap_ts = {T-1, int(0.7*T), int(0.4*T), 0}\n",
    "        snapshots = []\n",
    "        for t in range(T-1, -1, -1):\n",
    "            eps_pred = self.model(v, cond_onehot.argmax(dim=1), t)\n",
    "            v = self.noise_sched.restore_image(v, eps_pred, t, sigma=0.0)\n",
    "            if t in snap_ts:\n",
    "                a_hat_t = self.noise_sched.alpha_hat[t]\n",
    "                x0_pred = (v - torch.sqrt(1-a_hat_t)*eps_pred) / torch.sqrt(a_hat_t)\n",
    "                img = self.vae.decode(x0_pred.float()).detach()\n",
    "                snapshots.append((t, img.squeeze().cpu().numpy()))\n",
    "        cols = len(snapshots)\n",
    "        fig, axes = plt.subplots(1, cols, figsize=(cols*2, 2), dpi=150)\n",
    "        for i, (tt, im) in enumerate(snapshots):\n",
    "            axes[i].imshow(im)\n",
    "            axes[i].set_title(f\"t={tt}\", fontsize=8)\n",
    "            axes[i].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate = 16000,\n",
    "    n_fft = 800,\n",
    "    hop_length = 200,     \n",
    "    win_length  = 800,\n",
    "    n_mels = 80,\n",
    ")\n",
    "\n",
    "def wav_to_mel(wav):\n",
    "    # wav: (1, L)\n",
    "    spec = mel_spec(wav)      \n",
    "    spec = torch.log(spec + 1e-6)\n",
    "    T = spec.shape[-1]\n",
    "    if T < 80:                    \n",
    "        spec = F.pad(spec, (0, 80 - T))\n",
    "    elif T > 80:                  \n",
    "        spec = spec[..., :80]\n",
    "    return spec  \n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, csv_file, audio_dir, target_sr=16000, length_sec=None, transform=None):\n",
    "        self.table = pd.read_csv(csv_file)\n",
    "        self.audio_dir = Path(audio_dir)\n",
    "        self.sr = target_sr\n",
    "        self.length = int(target_sr * length_sec) if length_sec else None\n",
    "        self.transform = transform\n",
    "\n",
    "    def load_wav(self, path):\n",
    "        wav, sr = torchaudio.load(path)            \n",
    "        if wav.shape[0] > 1:        \n",
    "            wav = wav.mean(dim=0, keepdim=True)\n",
    "        if sr != self.sr:           \n",
    "            wav = torchaudio.functional.resample(wav, sr, self.sr)\n",
    "        return wav\n",
    "\n",
    "    def pad_trim(self, wav):\n",
    "        if self.length is None:\n",
    "            return wav\n",
    "        cur = wav.shape[-1]\n",
    "        if cur > self.length:\n",
    "            wav = wav[..., : self.length]\n",
    "        elif cur < self.length:\n",
    "            wav = pad(wav, (0, self.length - cur))\n",
    "        return wav\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rel = self.table.iloc[idx][\"file\"]\n",
    "        wav = self.load_wav(self.audio_dir / rel)\n",
    "        wav = self.pad_trim(wav).float()\n",
    "        mel = wav_to_mel(wav) \n",
    "\n",
    "        if self.transform:\n",
    "            mel = self.transform(mel)\n",
    "\n",
    "        return mel, 0         \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.table)\n",
    "\n",
    "\n",
    "def audio_collate(batch):\n",
    "    mels, labels = zip(*batch)           \n",
    "    return torch.stack(mels), torch.tensor(labels)                   \n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "def build_dataloader(cfg, split, transform=None, workers=4, limit=10_000):\n",
    "    d  = cfg[\"dataset\"][split]\n",
    "    ds = AudioDataset(\n",
    "        d[\"table\"], d[\"data\"],\n",
    "        cfg[\"vae\"][\"freq\"], cfg[\"vae\"][\"lenght\"],\n",
    "        transform,\n",
    "    )\n",
    "    if limit and limit < len(ds):\n",
    "        idx = np.random.choice(len(ds), limit, replace=False)\n",
    "        ds = torch.utils.data.Subset(ds, idx)    \n",
    "    sampler = None\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size = cfg[split][\"batch_size\"],\n",
    "        shuffle = (split == \"train\"),\n",
    "        sampler = sampler,\n",
    "        num_workers = workers,\n",
    "        pin_memory = cfg[split][\"pin_memory\"],\n",
    "        collate_fn = audio_collate,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "id": "RdbnEEr_WO3a"
   },
   "outputs": [],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "vae = VAE_Audio().to(device)\n",
    "params_vae = torch.load(str(Path.home() / \"Downloads\" / \"VaeAudio_V4_noise.pt\"), map_location=device, weights_only=True)\n",
    "vae.load_state_dict(params_vae)\n",
    "vae.eval()\n",
    "\n",
    "\n",
    "unet = MNIST_diffusion(input_channels=32, conditioning_length=10, timestamp_length=1, hidden_dims=128, mixin_dims=128).to(device)\n",
    "with torch.no_grad():\n",
    "    dummy = torch.zeros(1, 32, 8, 8, device=device)\n",
    "    _ = unet(dummy, mix=False, c=None, n=1)\n",
    "\n",
    "state = torch.load(Path.home() / \"Downloads\" / \"Unet_diffusion_v6.pt\", map_location=device, weights_only=True)\n",
    "unet.load_state_dict(state)\n",
    "unet.eval()\n",
    "\n",
    "tts_model = TTS_diffusion(\n",
    "    input_channels=32,\n",
    "    hidden_dims=128,\n",
    "    alphabet=alphabet,\n",
    "    emb_size_word=emb_size_word,\n",
    "    emb_size_noise=emb_size_noise,\n",
    "    noise_steps=noise_steps,\n",
    "    max_word_size=max_word_size,\n",
    "    max_time_size=2048,\n",
    ").to(device)\n",
    "\n",
    "train_csv = config['dataset']['train']['table']\n",
    "train_wav_dir = config['dataset']['train']['data']\n",
    "val_csv = config['dataset']['val']['table']\n",
    "val_wav_dir = config['dataset']['val']['data']\n",
    "\n",
    "train_dataloader = build_dataloader(config, \"train\", workers=0, limit=2500)\n",
    "val_dataloader = build_dataloader(config, \"val\", workers=0, limit=None)\n",
    "\n",
    "all_epochs = 10\n",
    "\n",
    "with torch.no_grad():\n",
    "    mel_zero = torch.zeros(1, 1, 80, 80, device=device)\n",
    "    z0, mu0, logvar0 = vae.sample(mel_zero)\n",
    "    C_lat, H_lat, W_lat = z0.shape[1:]\n",
    "    dummy_z = torch.zeros(1, C_lat, H_lat, W_lat, device=device)\n",
    "    fake_text_ids = torch.zeros(1, 1, dtype=torch.long, device=device) \n",
    "    fake_text_lens = torch.ones(1, dtype=torch.long, device=device)    \n",
    "    fake_t = torch.zeros(1, dtype=torch.long, device=device)\n",
    "    _ = unet(dummy_z, fake_text_ids, fake_text_lens, fake_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "1e4372de4580470ab76bee178110d9e2",
      "1abce761e8594caf861ac61d095d7ee0",
      "370b89768bc44e558fdba280b723b9e8",
      "3c0d9fd7a4d545a280cbbc3647d71fe0",
      "63015edadaff44859bccac493856be79",
      "2d77182ca6614893a22e5e97e89f89e8",
      "1bef6f7ecdcf41af973723d4e8d06042",
      "aeadc2b3988944bebacc38c58f35b613",
      "ca10df476c07485d89c8a53897b01ed6",
      "6f79cfac0261490cb3f29da58f32f1ac",
      "5fc1275d886e4af6859ef20979fd7d08",
      "6694aee399e94698b5e71bf8f62bc07c",
      "e6f491613b5c49758adc36f6deb5dd8c",
      "f18e1d16ac674e398aa3bd7f9798cbe5"
     ]
    },
    "id": "dZAMZa9_Wa0G",
    "outputId": "8fbba4c8-aa48-4b63-e22b-5f786ec2b899"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch: 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc1ba6c9b184857afeaa5369d482c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'text_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[183], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(all_epochs):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mall_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mval_loop(iteration\u001b[38;5;241m=\u001b[39mepoch)\n\u001b[0;32m     13\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(tts_model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTTSModelParameters_v6.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[180], line 81\u001b[0m, in \u001b[0;36mTTS_Trainer.train_loop\u001b[1;34m(self, KLDk, epoch)\u001b[0m\n\u001b[0;32m     78\u001b[0m cond_onehot \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mone_hot(cond, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     79\u001b[0m condition \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([cond_onehot, idx\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 81\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(model_input, \u001b[43mtext_ids\u001b[49m, idx)\n\u001b[0;32m     83\u001b[0m noise_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml2_loss(output, noise)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     84\u001b[0m kld_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae\u001b[38;5;241m.\u001b[39mKLD_loss(mu, logvar)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text_ids' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = TTS_Trainer(\n",
    "    model=tts_model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    vae=vae,\n",
    "    epochs=all_epochs\n",
    ")\n",
    "for epoch in range(all_epochs):\n",
    "    print(f\"\\n Epoch: {epoch + 1}/{all_epochs}\")\n",
    "    trainer.train_loop()\n",
    "    trainer.val_loop(iteration=epoch)\n",
    "\n",
    "torch.save(tts_model.state_dict(), \"TTSModelParameters_v6.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for TTS_diffusion:\n\tsize mismatch for input_scaler.0.weight: copying a param with shape torch.Size([128, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n\tsize mismatch for input_scaler.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for precode.0.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for precode.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for precode.2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for precode.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for precode.2.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for precode.2.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for precode.3.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for precode.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for output_scaler.0.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 32, 1, 1]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[172], line 87\u001b[0m\n\u001b[0;32m     75\u001b[0m model \u001b[38;5;241m=\u001b[39m TTS_diffusion(\n\u001b[0;32m     76\u001b[0m     input_channels   \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m,\n\u001b[0;32m     77\u001b[0m     hidden_dims      \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m     max_time_size    \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m,\n\u001b[0;32m     84\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     86\u001b[0m ckpt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTTSModelParameters_v6.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 87\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     89\u001b[0m text      \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mпривет, как дела?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2580\u001b[0m             ),\n\u001b[0;32m   2581\u001b[0m         )\n\u001b[0;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2587\u001b[0m         )\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TTS_diffusion:\n\tsize mismatch for input_scaler.0.weight: copying a param with shape torch.Size([128, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 128, 1, 1]).\n\tsize mismatch for input_scaler.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for precode.0.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for precode.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for precode.2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for precode.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for precode.2.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for precode.2.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for precode.3.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for precode.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for output_scaler.0.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 32, 1, 1])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from IPython.display import Audio\n",
    "import torchaudio\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Создаём энкодеры и расписатель шума\n",
    "#   Используйте те же гиперпараметры, что и при обучении!\n",
    "alphabet      = \"абвгдеёжзийклмнопрстуфхцчшщъыьэюя\"\n",
    "emb_size_word = 128\n",
    "max_word_size = 256\n",
    "word_enc      = Word_Encoder(alphabet, emb_size_word, max_word_size).to(device)\n",
    "\n",
    "emb_size_noise= 64\n",
    "noise_steps   = 1000\n",
    "noise_enc     = Noise_Encoder(emb_size_noise, noise_steps).to(device)\n",
    "\n",
    "noise_sched   = NoiseScheduler(noise_steps, all_epochs)\n",
    "noise_sched.alpha     = noise_sched.alpha.to(device)\n",
    "noise_sched.alpha_hat = noise_sched.alpha_hat.to(device)\n",
    "\n",
    "# 2) Вспомогательная функция для токенизации текста\n",
    "def text_to_tensor(text: str):\n",
    "    # метод tokenize уже возвращает тензор [1, L]\n",
    "    return word_enc.tokenize(text).to(device)\n",
    "\n",
    "# 3) Обратная диффузия (немного адаптированная под ваши классы)\n",
    "@torch.no_grad()\n",
    "def sample_mel(model, text_ids):\n",
    "    # text_ids: [1, L]\n",
    "    # 3.1 получаем эмбед текст+позиции\n",
    "    text_emb = word_enc(text_ids)             # [1, L, emb_size]\n",
    "\n",
    "    # 3.2 стартовый шум\n",
    "    # C_lat = скрытые каналы после VAE decoder, H_lat/W_lat — spatial\n",
    "    dummy = torch.zeros(1, 1, 80, 80, device=device)\n",
    "    C_lat, H_lat, W_lat = vae.sample(dummy)[0].shape[1:]\n",
    "    v = torch.randn(1, C_lat, H_lat, W_lat, device=device)\n",
    "\n",
    "    # 3.3 обратный проход\n",
    "    for t in range(noise_steps-1, -1, -1):\n",
    "        # формируем conditioning: [text_emb_flat | t]\n",
    "        # ваш ConditionMixingLayer принимает сначала x, потом c\n",
    "        t_tensor = torch.tensor([[t]], device=device)\n",
    "        cond = torch.cat([text_emb.mean(dim=1), t_tensor.float()], dim=1)  # [1, emb_size+1]\n",
    "\n",
    "        eps_pred = model(v, mix=True, c=cond, n=1)\n",
    "        v = noise_sched.restore_image(v, eps_pred, t, sigma=0.0)\n",
    "\n",
    "    # 3.4 декодим через VAE\n",
    "    mel = vae.decode(v)      # [1,1,80,80]\n",
    "    return torch.clamp(mel, min=0)\n",
    "\n",
    "# 4) Mel → waveform через Griffin-Lim\n",
    "def mel_to_waveform(mel):\n",
    "    # mel: [1,1,80,80]\n",
    "    mel = mel.squeeze(0)  # [1,80,80]\n",
    "    inv_mel = torchaudio.functional.inverse_mel_scale(\n",
    "        mel,\n",
    "        sample_rate=16000,\n",
    "        n_stft=800//2+1,\n",
    "        f_min=0,\n",
    "        f_max=8000\n",
    "    )\n",
    "    wav = torchaudio.functional.griffinlim(\n",
    "        inv_mel,\n",
    "        n_fft=800,\n",
    "        hop_length=200,\n",
    "        win_length=800,\n",
    "        n_iter=60\n",
    "    )\n",
    "    return wav\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 5) Пример запуска инференса\n",
    "model = TTS_diffusion(\n",
    "    input_channels   = 32,\n",
    "    hidden_dims      = 128,\n",
    "    alphabet         = alphabet,        # как при обучении\n",
    "    emb_size_word    = emb_size_word,   # 128\n",
    "    emb_size_noise   = emb_size_noise,  # 64\n",
    "    noise_steps      = noise_steps,     # 1000\n",
    "    max_word_size    = max_word_size,   # 256\n",
    "    max_time_size    = 2048,\n",
    ").to(device)\n",
    "\n",
    "ckpt = torch.load(\"TTSModelParameters_v6.pt\", map_location=\"cuda\", weights_only=True)\n",
    "model.load_state_dict(ckpt, strict=False)\n",
    "model.eval()\n",
    "text      = \"привет, как дела?\"\n",
    "text_ids  = text_to_tensor(text)         # [1, L]\n",
    "mel_pred  = sample_mel(model, text_ids)  # [1,1,80,80]\n",
    "wav_pred  = mel_to_waveform(mel_pred)    # [1, L_wave]\n",
    "\n",
    "# 6) Слушаем результат в ноутбуке\n",
    "Audio(wav_pred.cpu().squeeze(0).numpy(), rate=16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_scaler.0.weight', 'input_scaler.0.bias', 'output_scaler.0.weight', 'output_scaler.0.bias', 'output_scaler.2.weight', 'output_scaler.2.bias', 'precode.0.weight', 'precode.0.bias', 'precode.2.weight', 'precode.2.bias', 'precode.2.running_mean', 'precode.2.running_var', 'precode.2.num_batches_tracked', 'precode.3.weight', 'precode.3.bias', 'precode.5.weight', 'precode.5.bias', 'precode.5.running_mean', 'precode.5.running_var', 'precode.5.num_batches_tracked']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_8932\\764751671.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"TTSModelParameters_v6.pt\", map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load(\"TTSModelParameters_v6.pt\", map_location=\"cpu\")\n",
    "print(list(ckpt.keys())[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02ee7ca7bdef4b4b87af083475c1676a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0917c668bf704493880fc06eba149bcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a94653d25fb4e39bf10968ae5539510",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5e212209cc264763a9c9d772784fc3ee",
      "value": 0
     }
    },
    "172ddc57d72a4b44815669efe43dbb3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_29aa757761714d59910d871b4cd9ad7b",
      "max": 3750,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8c94bbea11594888882c81a30b429207",
      "value": 3049
     }
    },
    "1a00c32513e94f0cade529efdc7970b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e2b8a9ea3b0471991562f13e0f09332": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "29aa757761714d59910d871b4cd9ad7b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29f4325df69048b38943cb4102e4b206": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37ee1c64b7ef452a852f821ab6298432": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_97b66af426c44b05a555dc6dcac2f2f2",
      "placeholder": "​",
      "style": "IPY_MODEL_fe81edd7112a4960a103d1309368d21f",
      "value": "  0%"
     }
    },
    "3e715d546530473a8a5af642b0067b07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_02ee7ca7bdef4b4b87af083475c1676a",
      "placeholder": "​",
      "style": "IPY_MODEL_923de72f80c74171a37067ef0477a8a0",
      "value": "train, loss = 1.841:  81%"
     }
    },
    "4b01e294e1da4a289a42967fb4f65e5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c8685e41c56d441ea9a54d7204faa9d9",
      "placeholder": "​",
      "style": "IPY_MODEL_80973a38f66b455b85780085b4623adb",
      "value": " 0/10 [00:00&lt;?, ?it/s]"
     }
    },
    "4d2bac776b774b59ad679f24e1a2f0e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5caa7b5e2c0145e78991da80720ee832": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d69f451fe04c4a41a18920fe4ce56f64",
      "placeholder": "​",
      "style": "IPY_MODEL_d67d302be41a482086e05db0c04e2ef5",
      "value": " 3049/3750 [03:17&lt;00:43, 15.99it/s]"
     }
    },
    "5e212209cc264763a9c9d772784fc3ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "745c665a5bb74a9490acb360bdfdd503": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_37ee1c64b7ef452a852f821ab6298432",
       "IPY_MODEL_0917c668bf704493880fc06eba149bcc",
       "IPY_MODEL_4b01e294e1da4a289a42967fb4f65e5b"
      ],
      "layout": "IPY_MODEL_29f4325df69048b38943cb4102e4b206"
     }
    },
    "7a94653d25fb4e39bf10968ae5539510": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80973a38f66b455b85780085b4623adb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8c94bbea11594888882c81a30b429207": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8de95e223bc347e598e6cf351e075a57": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "923de72f80c74171a37067ef0477a8a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "97b66af426c44b05a555dc6dcac2f2f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ebbd2e572b547898bf5ecde2d3ea9c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a827198306294285844ed8666c878998": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f8c86823b74a43259368748140f32d89",
       "IPY_MODEL_f8b65c5be65a4f70b18461d08c2e3c33",
       "IPY_MODEL_c60030bebf684f798c24c6115150b4c8"
      ],
      "layout": "IPY_MODEL_9ebbd2e572b547898bf5ecde2d3ea9c7"
     }
    },
    "c60030bebf684f798c24c6115150b4c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e8ea8438fcea49c898a034032db96e42",
      "placeholder": "​",
      "style": "IPY_MODEL_e5c6a52210244a4cbd8451145d15968b",
      "value": " 876/3750 [00:50&lt;02:00, 23.87it/s]"
     }
    },
    "c8685e41c56d441ea9a54d7204faa9d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb05fc7c458c4a44b7c75c09f83c16da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d67d302be41a482086e05db0c04e2ef5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d69f451fe04c4a41a18920fe4ce56f64": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5c6a52210244a4cbd8451145d15968b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e8ea8438fcea49c898a034032db96e42": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f052f2f296854f7fb0e85e3832bd7d25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3e715d546530473a8a5af642b0067b07",
       "IPY_MODEL_172ddc57d72a4b44815669efe43dbb3e",
       "IPY_MODEL_5caa7b5e2c0145e78991da80720ee832"
      ],
      "layout": "IPY_MODEL_1a00c32513e94f0cade529efdc7970b3"
     }
    },
    "f8b65c5be65a4f70b18461d08c2e3c33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d2bac776b774b59ad679f24e1a2f0e2",
      "max": 3750,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8de95e223bc347e598e6cf351e075a57",
      "value": 876
     }
    },
    "f8c86823b74a43259368748140f32d89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cb05fc7c458c4a44b7c75c09f83c16da",
      "placeholder": "​",
      "style": "IPY_MODEL_1e2b8a9ea3b0471991562f13e0f09332",
      "value": "0 epoch, train, loss = 0.089:  23%"
     }
    },
    "fe81edd7112a4960a103d1309368d21f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
