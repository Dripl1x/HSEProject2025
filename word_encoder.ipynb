{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "class Word_Encoder(nn.Module): # Токенайзер + Эмбеддер для букв\n",
        "    def __init__(self, alphabet, emb_size, max_word_size = 256):\n",
        "        super().__init__()\n",
        "        self.alphabet = list(alphabet)+[\"<pad>\", \"<stress>\", \"<unk>\"] # буквы + спец токены: пустой, ударение и неизвестный символ\n",
        "        self.emb_size = emb_size\n",
        "        self.embeddings = nn.Embedding(len(self.alphabet), emb_size)\n",
        "        self.pos_embeddings = nn.Embedding(max_word_size, emb_size)\n",
        "        self.device = self.embeddings.device\n",
        "\n",
        "class Word_Encoder(nn.Module):\n",
        "    def __init__(self, alphabet, emb_size, max_word_size=256):\n",
        "        super().__init__()\n",
        "        self.alphabet = list(alphabet) + [\"<pad>\", \"<stress>\", \"<unk>\"]\n",
        "        self.emb_size = emb_size\n",
        "        self.max_word_size = max_word_size\n",
        "        self.embeddings = nn.Embedding(len(self.alphabet), emb_size)\n",
        "        self.pos_embeddings = nn.Embedding(max_word_size, emb_size)\n",
        "\n",
        "        self.get_idx = {char: idx for idx, char in enumerate(self.alphabet)}\n",
        "        self.pad_idx = self.get_idx[\"<pad>\"]\n",
        "        self.stress_idx = self.get_idx[\"<stress>\"]\n",
        "        self.unk_idx = self.get_idx[\"<unk>\"]\n",
        "        self.device = self.embeddings.weight.device\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        if isinstance(text, str):\n",
        "            text = [text]\n",
        "        tokenized = []\n",
        "        for word in text:\n",
        "            word_idxs = []\n",
        "            i = 0\n",
        "            n = len(word)\n",
        "            while i < n:\n",
        "                if word[i] == \"<\" and i + 8 < n and word[i:i+8] == \"<stress>\":\n",
        "                  word_idxs.append(self.stress_idx)\n",
        "                  i += 8\n",
        "                else:\n",
        "                    char = word[i]\n",
        "                    if char in self.get_idx:\n",
        "                        word_idxs.append(self.get_idx[char])\n",
        "                    else:\n",
        "                        word_idxs.append(self.unk_idx)\n",
        "                    i += 1\n",
        "\n",
        "            tokenized.append(word_idxs)\n",
        "        max_len = max(len(word) for word in tokenized)\n",
        "        padded = []\n",
        "        for word in tokenized:\n",
        "            padded_word = word\n",
        "            if len(word) < max_len:\n",
        "                padded_word += [self.pad_idx] * (max_len - len(word))\n",
        "            padded.append(padded_word)\n",
        "\n",
        "        return torch.tensor(padded, device=self.device)\n",
        "\n",
        "    def forward(self, x): # Не забыть проверить работу с батчами\n",
        "        self.device = x.device\n",
        "        n = x.shape[-1]\n",
        "        pos = torch.arange(n).to(self.device)\n",
        "        x = self.embeddings(x)+self.pos_embeddings(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "bwz9Ju6DkdX2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Word_Encoder(\"абвгдеёжзийклмнопрстуфхцчшщъыьэюя\", 128)\n",
        "print(encoder.get_idx)\n",
        "\n",
        "tokens1 = encoder.tokenize(\"привет\")\n",
        "print(tokens1)\n",
        "\n",
        "tokens2 = encoder.tokenize([\"прив<stress>ет\", \"м2ир\"])\n",
        "print(tokens2)\n",
        "\n",
        "tokens3 = encoder.tokenize(\"уратыработаешь\")\n",
        "print(tokens3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mktw6-Zfnl1S",
        "outputId": "46c5f58c-6fab-4c0f-f99d-7aab2d1a8a0a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'а': 0, 'б': 1, 'в': 2, 'г': 3, 'д': 4, 'е': 5, 'ё': 6, 'ж': 7, 'з': 8, 'и': 9, 'й': 10, 'к': 11, 'л': 12, 'м': 13, 'н': 14, 'о': 15, 'п': 16, 'р': 17, 'с': 18, 'т': 19, 'у': 20, 'ф': 21, 'х': 22, 'ц': 23, 'ч': 24, 'ш': 25, 'щ': 26, 'ъ': 27, 'ы': 28, 'ь': 29, 'э': 30, 'ю': 31, 'я': 32, '<pad>': 33, '<stress>': 34, '<unk>': 35}\n",
            "tensor([[16, 17,  9,  2,  5, 19]])\n",
            "tensor([[16, 17,  9,  2, 34,  5, 19],\n",
            "        [13, 35,  9, 17, 33, 33, 33]])\n",
            "tensor([[20, 17,  0, 19, 28, 17,  0,  1, 15, 19,  0,  5, 25, 29]])\n"
          ]
        }
      ]
    }
  ]
}