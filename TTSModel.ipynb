{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QCH5nYQmv1cA"
   },
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import torchvision.utils as vutils\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.cuda.amp import autocast\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from torch import amp\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, RandomSampler, Dataset\n",
    "from torch.optim.swa_utils import AveragedModel    \n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn.functional import pad\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b20J1mRIGR3-",
    "outputId": "1b8b0305-f163-415e-ed4f-e1c41b1c094d"
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6EsppzSwIKN"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"dataset\": {\n",
    "        \"train\": {\n",
    "            \"table\": \"E:/data/train.csv\",\n",
    "            \"data\": \"E:/data/bare_data/\"\n",
    "        },\n",
    "        \"val\": {\n",
    "            \"table\": \"E:/data/val.csv\",\n",
    "            \"data\": \"E:/data/bare_data/\"\n",
    "        }\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"batch_size\": 32,\n",
    "        \"grad_acum\": 1,\n",
    "        \"dtype\": \"float32\",\n",
    "        'shuffle': True,\n",
    "        'pin_memory': True,\n",
    "    },\n",
    "    \"val\": {\n",
    "        \"batch_size\": 32,\n",
    "        \"grad_acum\": 1,\n",
    "        \"dtype\": \"float32\",\n",
    "        'shuffle': False,\n",
    "        'pin_memory': True,\n",
    "    },\n",
    "    \"vae\": {\n",
    "        \"freq\": 16000,\n",
    "        \"lenght\": 5,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"latent_size\": 128,\n",
    "        \"epochs\": 15,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"freq_scale\": 4,\n",
    "        \"time_scale\": 4,\n",
    "    },\n",
    "    \"utils\": {\n",
    "        \"n_fft\": 800,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58r16JvWGR4B"
   },
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tU0zMRsTGR4B"
   },
   "outputs": [],
   "source": [
    "# Класс для перевода объекта в латентное пространство для упрощённой работы с ним, подаём объект -> получаем тензор меток на него, т.е. какими признаками он обладает и с помощью этого можем его сравнивать с другими и обучать модель.\n",
    "\n",
    "class VAE_Audio(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        # Слой получает признаки из исходного объекта\n",
    "        self.encoder_input = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.GELU(),\n",
    "            nn.Conv2d(64, 32, 3, 1, 1), nn.BatchNorm2d(32), nn.GELU(),\n",
    "        )\n",
    "        # Слой сжимает объект в латентное подпространство\n",
    "        self.encoder_squeeze = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, 3, 2, 1), nn.BatchNorm2d(32), nn.GELU(),\n",
    "            nn.Conv2d(32, 32, 3, 2, 1), nn.BatchNorm2d(32), nn.GELU(),\n",
    "        )\n",
    "        # Нужны, чтобы задать параметры Гауссовского распределения (mu - среднее, logvar - логарифм дисперсии)\n",
    "        self.encoder_mu = nn.Conv2d(32, 32, 1) # Набор меток, отвечающих за среднее значение признаков\n",
    "        self.encoder_logvar = nn.Conv2d(32, 32, 1) # Набор меток, отвечающих за то, как широко разбросаны признаки по латентному подпространству \n",
    "        \n",
    "        # Слои восстанавливают размерность до исходной\n",
    "        self.decoder_unsqueeze = nn.Sequential(\n",
    "                nn.ConvTranspose2d(32, 32, 3, 2, 1, output_padding=1), nn.BatchNorm2d(32), nn.GELU(),\n",
    "                nn.ConvTranspose2d(32, 32, 3, 2, 1, output_padding=1), nn.BatchNorm2d(32), nn.GELU(),\n",
    "        )\n",
    "        # Слой, отвечающий за выход: получаем объект, максимально похожий на исходный\n",
    "        self.decoder_output = nn.Sequential(\n",
    "                 nn.ConvTranspose2d(32, 16, 3, 1, 1), nn.GELU(),\n",
    "                 nn.BatchNorm2d(16),\n",
    "                 nn.ConvTranspose2d(16, 1, 3, 1, 1), \n",
    "        )\n",
    "    def encode(self, x): # Функция кодирования объекта в латентное подпространство (пространство меньшей размерности), получает параметры кодирования объекта (mu, logvar)\n",
    "        x = self.encoder_input(x)\n",
    "        x = self.encoder_squeeze(x)\n",
    "        mu = self.encoder_mu(x)\n",
    "        logvar = self.encoder_logvar(x)\n",
    "        return mu, logvar\n",
    "    def decode(self, x): # Декодирует объект в исходное пространство по набору признаков\n",
    "        x = self.decoder_unsqueeze(x)\n",
    "        x = self.decoder_output(x)\n",
    "        return x\n",
    "    def KLD_loss(self, mu, logvar, q=0.005): # Вычисляет Kullback-Leibler divergence (мера различия между двумя вероятностными распределениями) между предсказанным распределением и стандартным нормальным распределением\n",
    "        kld = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        kld = torch.clamp(kld, min=q)\n",
    "        return kld.mean()\n",
    "    def forward(self, x): # Полное прохождение через VAE\n",
    "        mu, logvar = self.encode(x)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        z = mu + std * torch.randn_like(std)\n",
    "        recon = self.decode(z)\n",
    "        return recon, z, mu, logvar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaLlnVbzwL9g"
   },
   "source": [
    "# Моделька"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJF9tFrIdzzT"
   },
   "outputs": [],
   "source": [
    "# Класс для постепенного зашумления картинки, чтобы диффузионная модель училась как можно лучше благодаря постепенному обучению (от меньшего шума к большему)\n",
    "\n",
    "class NoiseScheduler:\n",
    "    def __init__(self, timestamps=100, epochs=100):\n",
    "        self.steps = timestamps\n",
    "        self.epochs = epochs\n",
    "        self.betas = self.cosine_beta_schedule(timestamps)\n",
    "        self.alpha = 1 - self.betas\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "    \n",
    "    @staticmethod \n",
    "    def cosine_beta_schedule(timesteps, s=0.008): # Функция, которая с помощью косинуса определяет то, с какой скоростью будет происходить зашумление, т.е. более вогнутая вниз кривая, в отличие от других функций по типу линейных\n",
    "        steps = timesteps + 1\n",
    "        x = torch.linspace(0, timesteps, steps)\n",
    "        a_hat = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi / 2) ** 2\n",
    "        a_hat = a_hat / a_hat[0]\n",
    "        betas = 1 - (a_hat[1:] / a_hat[:-1])\n",
    "        return betas.clamp(1e-5, 0.999)\n",
    "        \n",
    "    def __call__(self, step): # Возвращает коэффициент зашумления (beta) на конкретном шаге\n",
    "        return self.betas[step]\n",
    "        \n",
    "    def add_noise(self, image, index): # Функция зашумления картинки, возвращает зашумлённый объект и сам шум\n",
    "        noise = torch.randn_like(image)\n",
    "        b = image.shape[0]\n",
    "        k = self.alpha_hat[index].view(b, 1, 1, 1)\n",
    "        return torch.sqrt(k) * image + torch.sqrt(1 - k) * noise, noise\n",
    "\n",
    "    def sample_timestamps(self, iters=10): # Выбирает случайные шаги зашумления для батча объектов\n",
    "        dev = self.alpha_hat.device\n",
    "        index = torch.randint(0, self.steps - 1, (iters - 1,), device=dev)\n",
    "        return torch.cat((index, torch.tensor([100], device=dev)))\n",
    "\n",
    "    def denoise_step(self, x_t, predicted_noise, t, sigma=0.0): # Функция обратной диффузии, т.е. расшумления картинки\n",
    "        random_noise = torch.randn_like(x_t)\n",
    "        if torch.is_tensor(t):\n",
    "            t_prev = torch.clamp(t - 1, min = 0)\n",
    "        else:\n",
    "            t_prev = max(t - 1, 0)\n",
    "        alpha_cumprod_t = self.alpha_hat[t]\n",
    "        alpha_cumprod_prev = self.alpha_hat[t_prev]\n",
    "        x0_pred = (x_t - torch.sqrt(1 - alpha_cumprod_t) * predicted_noise) / torch.sqrt(alpha_cumprod_t)\n",
    "        direction = torch.sqrt(1 - alpha_cumprod_prev) * predicted_noise\n",
    "        return torch.sqrt(alpha_cumprod_prev) * x0_pred + direction + sigma * random_noise\n",
    "\n",
    "    def get_index(self, epoch, batch_size): # Функция получения элемента из батча\n",
    "        return self.steps-torch.randint(0, self.steps, [batch_size]) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "raT7SjBAMMIY"
   },
   "outputs": [],
   "source": [
    "# Класс, в котором происходит объединение признаков, полученных от части изображения и условия на него, в итоге на изображении остаётся совокупность признаков с учтённым условием\n",
    "\n",
    "# conditions - условия, convolutions - признаки\n",
    "\n",
    "class ConditionMixingLayer(nn.Module):\n",
    "    def __init__(self, input_channels, conditioning_length, hidden_size = 8):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size                                          # Сколько признаков храним внутри для перемешивания признаков объекта и заданного условия\n",
    "        self.cond_proj = nn.Linear(conditioning_length, self.hidden_size)       # Переводит условие в скрытое пространство признаков нужной размерности\n",
    "        self.conv_proj = nn.Conv2d(input_channels, self.hidden_size, 3, 1, 1)   # Делает признаки и условие совместимыми для дальнейших операций\n",
    "        self.lin_proj = nn.Linear(self.hidden_size, self.hidden_size)           # Увеличивает гибкость/выразительность признаков\n",
    "        self.lin1_unproj = nn.Linear(self.hidden_size, self.hidden_size)        # Дополнительный проход №1 для объединения условий и признаков\n",
    "        self.lin2_unproj = nn.Linear(self.hidden_size, self.hidden_size)        # Дополнительный проход №2 для объединения условий и признаков\n",
    "        self.conv_unproj = nn.Conv2d(self.hidden_size, input_channels, 3, 1, 1) # Переводит смешанные признаки обратно в исходное число каналов\n",
    "        self.conv_act = nn.SiLU()                                               # Нужно для нелинейных преобразований признаков\n",
    "        self.bn1 = nn.BatchNorm2d(input_channels)                               # Стабилизируют распределение признаков\n",
    "        self.bn2 = nn.BatchNorm2d(input_channels)                               # Стабилизируют распределение признаков\n",
    "        self.add_a = nn.Linear(self.hidden_size, self.hidden_size)              # Независимые преобразования для признаков\n",
    "        self.add_b = nn.Linear(self.hidden_size, self.hidden_size)              # Независимые преобразования для условий\n",
    "\n",
    "    def forward(self, x, c=None, skip=False):\n",
    "        if len(x.shape)==3:\n",
    "            x = x.unsqueeze(0)\n",
    "        b, ch, h, w = x.shape # batch, channels, height, width\n",
    "        x = self.bn1(x)\n",
    "        xn = self.conv_proj(x) # [B, N, H, W]\n",
    "        xn = self.conv_act(xn)\n",
    "        xn = xn.view(b, h*w, self.hidden_size) # [B, H*W, N]\n",
    "        xn = self.lin_proj(xn) # [B, H*W, N]\n",
    "        xn = self.conv_act(xn) # [B, H*W, N]\n",
    "        if not skip:\n",
    "            cn = self.cond_proj(c) # [B, N]\n",
    "            cn = self.conv_act(cn) # [B, N]\n",
    "            cn = cn.view(b, 1, self.hidden_size)\n",
    "            xn = self.add_a(xn) # [B, 1, N]\n",
    "            cn = self.add_b(cn) # [B, H*W, N]\n",
    "            xn = xn + cn # [B, H*W, N]\n",
    "        xn = self.lin1_unproj(xn) # [B, H*W, N]\n",
    "        xn = self.conv_act(xn) # [B, H*W, N]\n",
    "        xn = self.lin2_unproj(xn) # [B, H*W, N]\n",
    "        xn = self.conv_act(xn) # [B, H*W, N]\n",
    "        xn = xn.view(b, self.hidden_size, h, w) # [B, N, H, W]\n",
    "        xn = self.conv_unproj(xn)\n",
    "        xn = self.conv_act(xn) # [B, I, H, W]\n",
    "        xn = self.bn2(xn)\n",
    "        x = xn + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vtu2x49DMU5R"
   },
   "outputs": [],
   "source": [
    "# Класс, который учится улучшать/дополнять исходные признаки. ВАЖНО: Отличие от MixingLayer в том, что в данном классе происходит улучшение и преобразование, а в Mixing происходит перемешиванние с конкретными признаками\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.norm1 = nn.GroupNorm(1, channels, affine=True)\n",
    "        self.act = nn.SiLU()\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.norm2 = nn.GroupNorm(1, channels, affine=True)\n",
    "        nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        nn.init.zeros_(self.conv1.bias)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight)\n",
    "        nn.init.zeros_(self.conv2.bias)\n",
    "        nn.init.ones_(self.norm1.weight)\n",
    "        nn.init.zeros_(self.norm1.bias)\n",
    "        nn.init.ones_(self.norm2.weight)\n",
    "        nn.init.zeros_(self.norm2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv1(x)\n",
    "        h = self.norm1(h)\n",
    "        h = self.act(h)\n",
    "        h = self.conv2(h)\n",
    "        h = self.norm2(h)\n",
    "        return x + h\n",
    "\n",
    "\n",
    "# Класс, который позволяет каждой позиции в данных учиться искать важную информацию. Позволяет одному патчу видеть признаки других, а не только соседних. Например, в нашем контексте одно слово ищет схожие для правильного понимания:\n",
    "# король + женщина = королева, и т.п.\n",
    "# Q - Query (Запрос), K - Key (Ключ), V - Value (Значение)\n",
    "# Query - что мне нужно узнать, чтобы получить как можно больше информации о себе?\n",
    "# Key - насколько я подхожу запросу?\n",
    "# Value - какая у меня есть информация для других запросов?\n",
    "# Cross-Attention - с добавлением условия (например текст об объекте), Self-Attention - на основе объекта\n",
    "class OurAttentionLayer(nn.Module):\n",
    "    def __init__(self, patch_size, channels_in, hidden_dim, emb_size=1, cross=False):\n",
    "        super().__init__()\n",
    "        self.cross = cross                                               # Флаг, есть ли внешнее условие\n",
    "        self.hidden_dim = hidden_dim                                     # Размерность Q, K, V\n",
    "        self.channels_in = channels_in                                   # Кол-во входных каналов\n",
    "        self.patch_size = patch_size                                     # Кол-во элементов в патче\n",
    "        self.emb_size = emb_size                                         # Размерность условия\n",
    "        self.key_proj = nn.Linear(patch_size, hidden_dim)                # Переводит каждый патч в скрытое пространство\n",
    "        self.value_proj = nn.Linear(patch_size, hidden_dim)              # Переводит каждый патч в скрытое пространство\n",
    "        self.norm = nn.LayerNorm([channels_in, patch_size])              # Нормализация для стабилизации выходных признаков\n",
    "        if cross:\n",
    "            self.cond_to_channel_proj = nn.Linear(emb_size, channels_in) # Переводит условие в пространство каналов\n",
    "            self.cond_to_attn_proj = nn.Linear(emb_size, hidden_dim)     # Переводит условие в пространство attention\n",
    "            self.query_proj = nn.Linear(hidden_dim, hidden_dim)          # Преобразует промежуточный query в финальный вектор запросов\n",
    "        else:\n",
    "            self.query_proj = nn.Linear(patch_size, hidden_dim)          # Проецирует запросы в пространство attention, и делает запросы там, т.к. нет внешнего условия\n",
    "        self.output_proj = nn.Linear(hidden_dim, patch_size)             # Переводит результат из пространства обратно в patch\n",
    "        self.softmax = nn.Softmax(dim=-1)                                # Для получения attention коэффициентов\n",
    "        self.dscale = 1 / (hidden_dim ** 0.5)                            # Множитель нормировки для устойчивости\n",
    "    def forward(self, image, text = None, ret_attn_qkv=False):\n",
    "        image = image.contiguous()                                       # Приведение к непрерывному виду\n",
    "        keys = self.key_proj(image)                                      # Описание каждого патча, чтобы понять, насколько он подходит запросу\n",
    "        values = self.value_proj(image)                                  # Хранит, какие значения передать при выборе данного изображения\n",
    "        if self.cross and text is None:                                  # Заглушка (если дали внешнее условие, а его нет, берём случайный объект текста)\n",
    "            text = torch.rand(1, self.emb_size, device=image.device)\n",
    "        if self.cross:\n",
    "            cond_channels = self.cond_to_channel_proj(text)              # Технические детали по преобразованию к нужному виду\n",
    "            cond_attention = self.cond_to_attn_proj(text)                # Технические детали по преобразованию к нужному виду\n",
    "            cond_query_mixed = torch.einsum(\"...sc,...sn->...cn\", cond_channels, cond_attention) # Смешанная матрица признаков\n",
    "            queries = self.query_proj(cond_query_mixed)                  # Технические детали по преобразованию к нужному виду\n",
    "            queries = queries.unsqueeze(2).unsqueeze(3).expand_as(keys)  # Технические детали по преобразованию к нужному виду\n",
    "        else:\n",
    "            queries = self.query_proj(image)\n",
    "        attn_scores = torch.einsum(\"...jn,...cn->...cj\", queries, keys)         # Считаем, насколько запрос похож на ключ\n",
    "        attn_weights = self.softmax(attn_scores * self.dscale)                  # Нормировка\n",
    "        attn_output = torch.einsum(\"...ic,...cn->...in\", attn_weights, values)  # Берём взвешенные метки values\n",
    "        output = self.output_proj(attn_output)                                  # Переводим результат обратно в патч\n",
    "        output = output + image\n",
    "        output = output.permute(0, 2, 3, 1, 4)\n",
    "        output = self.norm(output)\n",
    "        output = output.permute(0, 3, 1, 2, 4)\n",
    "        if ret_attn_qkv:\n",
    "            return output, queries, keys, values\n",
    "        return output\n",
    "\n",
    "# Класс для обработки картинки поблочно, для удобства нужно преобразование к квадратичной форме\n",
    "class PatchImage(nn.Module):\n",
    "    def __init__(self, patch_size, reverse=False):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size         # Число элементов в одном блоке\n",
    "        self.n = int(self.patch_size**(0.5)) # Размер стороны блока\n",
    "        assert (self.n ** 2) == patch_size, \"Size isn't a full square!\"\n",
    "        self.reverse = reverse\n",
    "    def forward(self, x):\n",
    "        n = self.n\n",
    "        if self.reverse:\n",
    "            b, c, h, w, s = x.shape\n",
    "            x = torch.reshape(x, (b, c, h, w, n, n))\n",
    "            x = torch.transpose(x, -2, -3)\n",
    "            x = torch.reshape(x, (b, c, h * n, w * n))\n",
    "            return x\n",
    "        b, c, h, w = x.shape\n",
    "        x = torch.reshape(x, (b, c, h // n, n, w // n, n))\n",
    "        x = torch.transpose(x, -2, -3)\n",
    "        x = torch.reshape(x, (b, c, h // n, w // n, n * n))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwz9Ju6DkdX2"
   },
   "outputs": [],
   "source": [
    "class Word_Encoder(nn.Module):\n",
    "    def __init__(self, alphabet, emb_size, max_word_size=256):\n",
    "        super().__init__()\n",
    "        self.alphabet = list(alphabet) + [\"<pad>\", \"<stress>\", \"<unk>\"]\n",
    "        self.emb_size = emb_size\n",
    "        self.max_word_size = max_word_size\n",
    "        self.embeddings = nn.Embedding(len(self.alphabet), emb_size)\n",
    "        self.pos_embeddings = nn.Embedding(max_word_size, emb_size)\n",
    "\n",
    "        self.get_idx = {char: idx for idx, char in enumerate(self.alphabet)}\n",
    "        self.pad_idx = self.get_idx[\"<pad>\"]\n",
    "        self.stress_idx = self.get_idx[\"<stress>\"]\n",
    "        self.unk_idx = self.get_idx[\"<unk>\"]\n",
    "        self.device = self.embeddings.weight.device\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        if isinstance(text, str):\n",
    "            text = [text]\n",
    "        tokenized = []\n",
    "        for word in text:\n",
    "            word_idxs = []\n",
    "            i = 0\n",
    "            n = len(word)\n",
    "            while i < n:\n",
    "                if word[i] == \"<\" and i + 8 < n and word[i:i+8] == \"<stress>\":\n",
    "                  word_idxs.append(self.stress_idx)\n",
    "                  i += 8\n",
    "                else:\n",
    "                    char = word[i]\n",
    "                    if char in self.get_idx:\n",
    "                        word_idxs.append(self.get_idx[char])\n",
    "                    else:\n",
    "                        word_idxs.append(self.unk_idx)\n",
    "                    i += 1\n",
    "\n",
    "            tokenized.append(word_idxs)\n",
    "        max_len = max(len(word) for word in tokenized)\n",
    "        padded = []\n",
    "        for word in tokenized:\n",
    "            padded_word = word\n",
    "            if len(word) < max_len:\n",
    "                padded_word += [self.pad_idx] * (max_len - len(word))\n",
    "            padded.append(padded_word)\n",
    "\n",
    "        return torch.tensor(padded, dtype=torch.long, device=self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, n = x.shape\n",
    "        pos = torch.arange(n, device=x.device).unsqueeze(0).expand(batch, n)\n",
    "        x = self.embeddings(x) + self.pos_embeddings(pos)\n",
    "        return x\n",
    "\n",
    "# Класс, с помощью которого модель определяет, насколько сильное сейчас зашумление, т.е. для каждого шага зашумления она хранит набор признаков\n",
    "class Noise_Encoder(nn.Module):\n",
    "    def __init__(self, emb_size, timestamps = 1000):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(timestamps, emb_size)\n",
    "    def forward(self, x):\n",
    "        self.device = x.device\n",
    "        return self.embeddings(x)\n",
    "\n",
    "# Класс, с помощью которого модель определяет, где каждый элемент находится во времени (нужно, чтобы понимать где конец, а где начало слова, или где звук по времени)\n",
    "class Time_Encoder(nn.Module): \n",
    "    def __init__(self, in_channels, out_channels, max_time_size=1024,): \n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, (3, 7), (1, 3), (1, 3))\n",
    "        self.pos_embs =  nn.Embedding(max_time_size, out_channels)\n",
    "        self.max_time_size = max_time_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, (3, 7), (1, 3), (1, 3))\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels, (3, 7), padding=(1, 3))\n",
    "        self.act = nn.SiLU()\n",
    "    def forward(self, image):\n",
    "        x = self.conv1(image)\n",
    "        b, c, h, w = x.shape\n",
    "        y = self.conv2(image)\n",
    "        time = torch.arange(w, device=image.device).expand(b, w)\n",
    "        pos = self.pos_embs(time) # [b, w, out]\n",
    "        pos = torch.permute(pos, [0, 2, 1]).unsqueeze(1)\n",
    "        x = torch.permute(x, [0, 2, 1, 3])\n",
    "        pos = pos.expand_as(x)\n",
    "        x = x + pos\n",
    "        y = self.act(y)\n",
    "        x = torch.permute(x, [0, 2, 1, 3])\n",
    "        z = self.conv3(x) + y\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "giy9WAb2wOTx"
   },
   "outputs": [],
   "source": [
    "# Класс для хранения синусоидных меток о позиции во времени\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, t):\n",
    "        half = self.dim // 2\n",
    "        emb = math.log(10000) / (half - 1)\n",
    "        freqs = torch.exp(torch.arange(half, device=t.device) * -emb)\n",
    "        pts = t.float().unsqueeze(1) * freqs.unsqueeze(0)\n",
    "        return torch.cat([pts.sin(), pts.cos()], dim=1)\n",
    "\n",
    "\n",
    "class TTS_diffusion(nn.Module):\n",
    "    def __init__(self, input_channels = 1, hidden_dims = 32, alphabet = \"абвгдеёжзийклмнопрстуфхцчшщъыьэюя\", emb_size_word = 128, emb_size_noise = 64,\n",
    "                       noise_steps = 100, max_word_size = 256, max_time_size = 2048):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_scaler = nn.Conv2d(input_channels, hidden_dims, kernel_size=1)  # Сворачивает mel спектрограмму до нужного числа каналов                      \n",
    "        self.precode = nn.Sequential(  # Набор слоёв, которые сначала сжимают, а потом разжимают изображение попутно добавляя информацию\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.SiLU(), nn.GroupNorm(8, hidden_dims),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 2, 1), nn.GroupNorm(8, hidden_dims), nn.SiLU(),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 2, 1), nn.GroupNorm(8, hidden_dims), nn.SiLU(),\n",
    "                nn.ConvTranspose2d(hidden_dims, hidden_dims, 3, 2, 1, output_padding=1), nn.GroupNorm(8, hidden_dims), nn.SiLU(),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.SiLU(), nn.GroupNorm(8, hidden_dims),\n",
    "                nn.ConvTranspose2d(hidden_dims, hidden_dims, 3, 2, 1, output_padding=1), nn.GroupNorm(8, hidden_dims), nn.SiLU(),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.SiLU(), nn.GroupNorm(8, hidden_dims),\n",
    "            )\n",
    "        \n",
    "        self.time_enc = Time_Encoder(hidden_dims, hidden_dims, max_time_size) # Каждому столбцу даёт позиционную метку\n",
    "        self.post_time =  nn.Sequential(\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.SiLU(), nn.GroupNorm(8, hidden_dims),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.SiLU(), nn.GroupNorm(8, hidden_dims),\n",
    "        )\n",
    "        \n",
    "        self.word_enc = Word_Encoder(alphabet, emb_size_word, max_word_size) # Переводит текст в вектор индексов\n",
    "        self.noise_enc = Noise_Encoder(emb_size_noise, noise_steps) # Признаки на шаге диффузии\n",
    "\n",
    "        # Далее у нас есть несколько слоёв attention, формально первый из них (шумовой) ищет на каком шаге находятся зашумления, и как это должно повлиять на расшумление\n",
    "        # Второй attention ищет непосредственно связи в тексте, изучая, какой звук сопоставить\n",
    "        # Также есть self attention, чтобы патчи внутри повзаимодействовали сами с собой\n",
    "        # После этого идёт обратное преобразование патчей в целое изображение\n",
    "        self.patch_img1_size = 16\n",
    "        self.patch_img1 = PatchImage(self.patch_img1_size)\n",
    "        self.atten_noise1 = OurAttentionLayer(self.patch_img1_size, hidden_dims, hidden_dims, emb_size_noise, True)\n",
    "        self.atten_word1 = OurAttentionLayer(self.patch_img1_size, hidden_dims, hidden_dims, emb_size_word, True)\n",
    "        self.atten_word2 = OurAttentionLayer(self.patch_img1_size, hidden_dims, hidden_dims, emb_size_word, True)\n",
    "        self.satten1 = OurAttentionLayer(self.patch_img1_size, hidden_dims, hidden_dims, 1, False)\n",
    "        self.unpatch_img1 = PatchImage(self.patch_img1_size, True)\n",
    "        self.main_block1 = nn.Sequential(\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.SiLU(), nn.GroupNorm(8, hidden_dims),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 2, 1), nn.GroupNorm(8, hidden_dims), nn.SiLU(),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 2, 1), nn.GroupNorm(8, hidden_dims), nn.SiLU(),\n",
    "            )\n",
    "        self.patch_img2_size = 16\n",
    "        self.patch_img2 = PatchImage(self.patch_img2_size)\n",
    "        self.atten_word3 = OurAttentionLayer(self.patch_img2_size, hidden_dims, hidden_dims, emb_size_word, True)\n",
    "        self.satten2 = OurAttentionLayer(self.patch_img2_size, hidden_dims, hidden_dims, 1, False)\n",
    "        self.atten_word4 = OurAttentionLayer(self.patch_img2_size, hidden_dims, hidden_dims, emb_size_word, True)\n",
    "        self.unpatch_img2 = PatchImage(self.patch_img2_size, True)\n",
    "        self.main_block2 = nn.Sequential(\n",
    "                nn.ConvTranspose2d(hidden_dims, hidden_dims, 3, 2, 1, output_padding=1), nn.GroupNorm(8, hidden_dims), nn.SiLU(),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.SiLU(), nn.GroupNorm(8, hidden_dims),\n",
    "                nn.ConvTranspose2d(hidden_dims, hidden_dims, 3, 2, 1, output_padding=1), nn.GroupNorm(8, hidden_dims), nn.SiLU(),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.SiLU(), nn.GroupNorm(8, hidden_dims),\n",
    "        )\n",
    "        self.patch_img3_size = 16\n",
    "        self.patch_img3 = PatchImage(self.patch_img3_size)\n",
    "        self.atten_noise2 = OurAttentionLayer(self.patch_img3_size, hidden_dims, hidden_dims, emb_size_noise, True)\n",
    "        self.atten_word5 = OurAttentionLayer(self.patch_img3_size, hidden_dims, hidden_dims, emb_size_word, True)\n",
    "        self.atten_word6 = OurAttentionLayer(self.patch_img3_size, hidden_dims, hidden_dims, emb_size_word, True)\n",
    "        self.satten3 = OurAttentionLayer(self.patch_img3_size, hidden_dims, hidden_dims, 1, False)\n",
    "        self.unpatch_img3 = PatchImage(self.patch_img3_size, True)\n",
    "        self.main_block3 = nn.Sequential(\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.SiLU(), nn.GroupNorm(8, hidden_dims),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.SiLU(), nn.GroupNorm(8, hidden_dims),\n",
    "                nn.Conv2d(hidden_dims, hidden_dims, 3, 1, 1), nn.SiLU(), nn.GroupNorm(8, hidden_dims),\n",
    "        )\n",
    "        self.output_scaler = nn.Conv2d(hidden_dims, input_channels, kernel_size=1)\n",
    "        self.out_scaler_conv1 = nn.Conv2d(in_channels=hidden_dims, out_channels=input_channels, kernel_size=1)\n",
    "        self.out_scaler_conv2 = nn.Conv2d(in_channels=input_channels, out_channels=input_channels, kernel_size=1)\n",
    "    def drop(self, layer, x, *args):\n",
    "        if self.training and torch.rand(1, device=x.device).item() < 0.05:\n",
    "            return x\n",
    "        return layer(x, *args)\n",
    "    def forward(self, x, text, noise):\n",
    "        B, C, H0, W0 = x.shape\n",
    "        x = self.input_scaler(x)\n",
    "        words = self.word_enc(text)\n",
    "        sh = self.noise_enc(noise)\n",
    "        if sh.dim() == 2:\n",
    "            sh = sh.unsqueeze(1)\n",
    "        x = self.time_enc(x)\n",
    "        x = self.post_time(x)\n",
    "        x = self.precode(x)\n",
    "        \n",
    "        x = self.resize_to_square(x, self.patch_img1_size)\n",
    "        x = self.patch_img1(x)\n",
    "        x = self.drop(self.atten_noise1, x, sh) # ✓ p = 0.1\n",
    "        x = self.atten_word1(x, words)\n",
    "        x = self.atten_word2(x, words)\n",
    "        x = self.satten1(x)\n",
    "        x = self.unpatch_img1(x)\n",
    "        x = self.main_block1(x)\n",
    "\n",
    "        x = self.resize_to_square(x, self.patch_img2_size)\n",
    "        x = self.patch_img2(x)\n",
    "        x = self.atten_word3(x, words)\n",
    "        x = self.atten_word4(x, words)\n",
    "        x = self.satten2(x)\n",
    "        x = self.unpatch_img2(x)\n",
    "        x = self.main_block2(x)\n",
    "        x = self.resize_to_square(x, self.patch_img3_size)\n",
    "        x = self.patch_img3(x)\n",
    "        x = self.drop(self.atten_noise2, x, sh) # ✓ p = 0.1\n",
    "        x = self.atten_word5(x, words)\n",
    "        x = self.atten_word6(x, words)\n",
    "        x = self.satten3(x)\n",
    "        x = self.unpatch_img3(x)\n",
    "        x = self.main_block3(x)\n",
    "        \n",
    "        x = self.output_scaler(x)\n",
    "        x = torch.nn.functional.interpolate(x, size=(H0, W0), mode='bilinear', align_corners=False)\n",
    "        return x\n",
    "    def resize_to_square(self, x, patch_size):\n",
    "        n = int(math.sqrt(patch_size))\n",
    "        B,C,H,W = x.shape\n",
    "        s = max(H, W)\n",
    "        s = ((s + n - 1) // n) * n\n",
    "        return F.interpolate(x, (s, s), mode='bilinear', align_corners=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45dnwlxUwSRg"
   },
   "source": [
    "# Тренер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNu1TxO0wTvp"
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "from torch.optim.swa_utils import AveragedModel\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import random\n",
    "\n",
    "class AverageMeter:\n",
    "    def __init__(self,):\n",
    "        self.arr = []\n",
    "    def __call__(self, item, n = 1):\n",
    "        if n <= 1:\n",
    "            self.arr.extend([item])\n",
    "        else:\n",
    "            self.arr.extend([item] * n)\n",
    "    def __str__(self,) -> str:\n",
    "        return str(np.mean(np.array(self.arr)))\n",
    "    def zero(self,):\n",
    "        self.arr=[]\n",
    "\n",
    "\n",
    "class TTS_Trainer:\n",
    "    def __init__(self, model, vae, train_dl, val_dl, noise_steps=100, epochs=100):\n",
    "        self.model, self.vae = model, vae\n",
    "        self.tdl, self.vdl = train_dl, val_dl\n",
    "        self.epochs = epochs\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.opt = torch.optim.AdamW(model.parameters(), lr=1e-4, betas=(0.9,0.99))\n",
    "        warm = torch.optim.lr_scheduler.LinearLR(self.opt, 0.2, 1.0, total_iters=2000)\n",
    "        decay = torch.optim.lr_scheduler.CosineAnnealingLR(self.opt, T_max=len(train_dl)*epochs, eta_min=1e-5)\n",
    "        self.lr_sched = torch.optim.lr_scheduler.SequentialLR(self.opt, [warm, decay], milestones=[2000])\n",
    "        self.noise_sched = NoiseScheduler(noise_steps, epochs)\n",
    "        self._set_cosine_schedule(noise_steps)\n",
    "        self.noise_sched.alpha_hat = self.noise_sched.alpha_hat.to(self.device)\n",
    "        self.ema = AveragedModel(self.model, avg_fn=lambda e,p,_: e*0.999 + p*0.001)\n",
    "        \n",
    "    def _set_cosine_schedule(self, steps, s: float = 0.008):\n",
    "        t = torch.arange(steps+1, dtype=torch.float32) / steps\n",
    "        alphas = torch.cos((t + s) / (1 + s) * math.pi / 2) ** 2\n",
    "        alphas = alphas / alphas[0]\n",
    "        betas = 1 - (alphas[1:] / alphas[:-1])\n",
    "        betas = betas.clamp(max=0.999)\n",
    "        self.noise_sched.betas = betas.to(self.device)\n",
    "        self.noise_sched.alpha = 1.0 - betas\n",
    "        self.noise_sched.alpha_hat = torch.cumprod(self.noise_sched.alpha, dim=0)\n",
    "        print(\"β[:10] =\", betas[:10].cpu().numpy())\n",
    "\n",
    "    def _q_sample(self, z0, t, eps):\n",
    "        a = self.noise_sched.alpha_hat[t].view(-1,1,1,1)\n",
    "        return a.sqrt()*z0 + (1-a).sqrt()*eps\n",
    "\n",
    "    def train_loop(self, epoch=0):\n",
    "        self.model.train();  self.vae.eval()\n",
    "        for p in self.vae.parameters(): p.requires_grad_(False)\n",
    "        pbar = tqdm(self.tdl, desc=f\"Train {epoch:02d}\", ncols=1440)\n",
    "        cnt = 0\n",
    "        for mel, texts in pbar:\n",
    "            B = mel.size(0)\n",
    "            mel = mel.to(self.device).float()\n",
    "            with torch.no_grad():\n",
    "                mu, logvar = self.vae.encode(mel)\n",
    "                std = torch.exp(0.5 * logvar)\n",
    "                z = mu + std * torch.randn_like(std)\n",
    "                z = z - z.mean(dim=[2,3], keepdim=True)\n",
    "            t = torch.randint(0, self.noise_sched.steps, (B,), device=self.device)\n",
    "            eps = torch.randn_like(mu)\n",
    "            x_t = self._q_sample(z, t, eps)\n",
    "            ids = self.model.word_enc.tokenize(texts).to(self.device)\n",
    "            input_for_model = x_t\n",
    "            target = eps\n",
    "            pred = self.model(input_for_model, ids, t)\n",
    "            if pred.shape != target.shape:\n",
    "                pred = F.interpolate(pred, size=target.shape[-2:], mode=\"bilinear\")\n",
    "            beta_t = self.noise_sched.betas[t].view(-1,1,1,1)   # если хочешь вариант β²\n",
    "            w_t = beta_t * beta_t   \n",
    "            mse = ((pred - target)**2 * w_t).mean()\n",
    "            l2 = mse.sqrt()\n",
    "            loss = mse\n",
    "            cnt += 1\n",
    "            self.opt.zero_grad()\n",
    "            loss.backward()\n",
    "            total_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            if (cnt % 100 == 0):\n",
    "                print(f\"step {cnt:4d}  loss (MSE): {loss.item()}   L2: {l2.item()}   grad_norm: {total_norm}\")\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.opt.step()\n",
    "            self.lr_sched.step()\n",
    "            self.ema.update_parameters(self.model)\n",
    "            pbar.set_postfix(mse=f\"{loss.item()}\", l2=f\"{l2.item()}\")\n",
    "        pbar.close()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def val_loop(self, max_batches=40):\n",
    "        self.model.eval();  self.vae.eval()\n",
    "        tot_mse, tot_l2, n = 0.0, 0.0, 0\n",
    "        for i,(mel,texts) in enumerate(self.vdl):\n",
    "            if i >= max_batches: break\n",
    "            B = mel.size(0);  mel = mel.to(self.device).float()\n",
    "            mu, logvar = self.vae.encode(mel)\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            z = mu + std * torch.randn_like(std)\n",
    "            z = z - z.mean(dim=[2,3], keepdim=True)\n",
    "            t = torch.randint(0, self.noise_sched.steps, (B,), device=self.device)\n",
    "            eps = torch.randn_like(mu)\n",
    "            x_t = self._q_sample(z, t, eps)\n",
    "            pred = self.model(x_t, self.model.word_enc.tokenize(texts).to(self.device), t)\n",
    "            if pred.shape != eps.shape:\n",
    "                pred = F.interpolate(pred, eps.shape[-2:], mode=\"bilinear\")\n",
    "            beta_t = self.noise_sched.betas[t].view(-1,1,1,1)   # если хочешь вариант β²\n",
    "            w_t = beta_t * beta_t   \n",
    "            mse = ((pred - eps)**2 * w_t).mean().item()\n",
    "            l2 = mse ** 0.5\n",
    "            tot_mse += mse * B\n",
    "            tot_l2 += l2 * B\n",
    "            n += B\n",
    "        val_mse = tot_mse / n if n else float(\"nan\")\n",
    "        val_l2 = tot_l2 / n if n else float(\"nan\")\n",
    "        print(f\"[val] ε-L2 = {val_l2}   MSE = {val_mse}\")\n",
    "        return val_mse, val_l2\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def draw_diffusion(self, save=\"diff.png\", steps=(199,140,80,0)):\n",
    "        self.ema.eval();  self.vae.eval()\n",
    "        mel0, _ = next(iter(self.vdl))\n",
    "        mel0 = mel0.to(self.device).float()\n",
    "        mu,_ = self.vae.encode(mel0)\n",
    "        C, H, W = mu.shape[1:]\n",
    "        v = torch.randn(1, C, H, W, device=self.device)\n",
    "        imgs = []\n",
    "        for t in reversed(range(self.noise_sched.steps)):\n",
    "            eps = self.ema.module(v, torch.zeros(1,1,dtype=torch.long,device=self.device), torch.tensor([t], device=self.device))\n",
    "            v = self.noise_sched.denoise_step(v, eps, t, sigma=0.0)\n",
    "            if t in steps:\n",
    "                imgs.append((t, self.vae.decode(v).squeeze().cpu()))\n",
    "        cols = len(imgs)\n",
    "        fig, axes=plt.subplots(1, cols, figsize=(2 * cols, 2))\n",
    "        for i, (tt, im) in enumerate(imgs):\n",
    "            axes[i].imshow(im,aspect='auto')\n",
    "            axes[i].set_title(f\"t={tt}\"); axes[i].axis('off')\n",
    "        plt.tight_layout();  plt.savefig(save); plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate = 16000,\n",
    "    n_fft = 800,\n",
    "    hop_length = 200,     \n",
    "    win_length  = 800,\n",
    "    n_mels = 80,\n",
    ")\n",
    "\n",
    "\n",
    "def wav_to_mel(wav):\n",
    "    spec = mel_spec(wav)      \n",
    "    spec = torch.log(spec + 1e-6)\n",
    "    T = spec.shape[-1]\n",
    "    MAX_FRAMES = 80\n",
    "    if T < MAX_FRAMES:\n",
    "        spec = F.pad(spec, (0, MAX_FRAMES - T))\n",
    "    else:\n",
    "        spec = spec[..., :MAX_FRAMES]\n",
    "    return spec  \n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, csv_file, audio_dir, target_sr=16000, length_sec=None, transform=None):\n",
    "        self.table = pd.read_csv(csv_file)\n",
    "        self.audio_dir = Path(audio_dir)\n",
    "        self.sr = target_sr\n",
    "        self.length = int(target_sr * length_sec) if length_sec else None\n",
    "        self.transform = transform\n",
    "\n",
    "    def load_wav(self, path):\n",
    "        wav, sr = torchaudio.load(path)\n",
    "        if wav.shape[0] > 1:\n",
    "            wav = wav.mean(dim=0, keepdim=True)\n",
    "        if sr != self.sr:\n",
    "            wav = torchaudio.functional.resample(wav, sr, self.sr)\n",
    "        return wav\n",
    "\n",
    "    def pad_trim(self, wav):\n",
    "        if self.length is None:\n",
    "            return wav\n",
    "        cur = wav.shape[-1]\n",
    "        if cur > self.length:\n",
    "            wav = wav[..., : self.length]\n",
    "        elif cur < self.length:\n",
    "            wav = torch.nn.functional.pad(wav, (0, self.length - cur))\n",
    "        return wav\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.table.iloc[idx]\n",
    "        rel_path = row[\"path\"]\n",
    "        text = row[\"sentence\"]\n",
    "        wav = self.load_wav(self.audio_dir / rel_path)\n",
    "        wav = self.pad_trim(wav).float()\n",
    "        mel = wav_to_mel(wav)\n",
    "        if self.transform:\n",
    "            mel = self.transform(mel)\n",
    "        return mel, text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.table)\n",
    "\n",
    "\n",
    "def audio_collate(batch):\n",
    "    mels, texts = zip(*batch)\n",
    "    return torch.stack(mels), list(texts)                 \n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "def build_dataloader(cfg, split, transform=None, workers=4, limit=10_000):\n",
    "    d  = cfg[\"dataset\"][split]\n",
    "    ds = AudioDataset(\n",
    "        d[\"table\"], d[\"data\"],\n",
    "        cfg[\"vae\"][\"freq\"], cfg[\"vae\"][\"lenght\"],\n",
    "        transform,\n",
    "    )\n",
    "    if limit and limit < len(ds):\n",
    "        idx = np.random.choice(len(ds), limit, replace=False)\n",
    "        ds = torch.utils.data.Subset(ds, idx)    \n",
    "    sampler = None\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size = cfg[split][\"batch_size\"],\n",
    "        shuffle = (split == \"train\"),\n",
    "        sampler = sampler,\n",
    "        num_workers = workers,\n",
    "        pin_memory = cfg[split][\"pin_memory\"],\n",
    "        collate_fn = audio_collate,\n",
    "    )\n",
    "def compute_mel_mean_std(dataloader):\n",
    "    mel_sum, mel_sq_sum, n_elem = 0.0, 0.0, 0\n",
    "    for mel, _ in tqdm(dataloader):\n",
    "        mel = mel.float()\n",
    "        mel_sum += mel.sum().item()\n",
    "        mel_sq_sum += (mel ** 2).sum().item()\n",
    "        n_elem += mel.numel()\n",
    "    mean = mel_sum / n_elem\n",
    "    var = mel_sq_sum / n_elem - mean ** 2\n",
    "    std = var ** 0.5\n",
    "    return mean, std\n",
    "\n",
    "train_dl = build_dataloader(config, \"train\", transform=None, workers=0, limit=10000)\n",
    "mel_mean, mel_std = compute_mel_mean_std(train_dl)\n",
    "print(f\"MEL mean: {mel_mean}, MEL std: {mel_std}\")\n",
    "\n",
    "class MelNormalize:\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    def __call__(self, mel):\n",
    "        return (mel - self.mean) / self.std\n",
    "        \n",
    "mel_norm = MelNormalize(mel_mean, mel_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdbnEEr_WO3a"
   },
   "outputs": [],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "vae = VAE_Audio().to(device)\n",
    "params_vae = torch.load(str(Path.home() / \"Downloads\" / \"TTSVAE_v4.5.pt\"), map_location=device, weights_only=True)\n",
    "vae.load_state_dict(params_vae)\n",
    "vae.eval()\n",
    "\n",
    "train_dataloader = build_dataloader(config, \"train\", transform = mel_norm, workers=0, limit=10000)\n",
    "val_dataloader = build_dataloader(config, \"val\", transform = mel_norm, workers=0, limit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.inference.vocoders import HIFIGAN\n",
    "from speechbrain.utils.fetching import LocalStrategy\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "vocoder = HIFIGAN.from_hparams(\n",
    "    source=\"speechbrain/tts-hifigan-libritts-16kHz\",\n",
    "    run_opts={\"device\": DEVICE},\n",
    "    savedir=\"pretrained_models/hifigan_16k_80\",\n",
    "    local_strategy=LocalStrategy.COPY\n",
    ").to(DEVICE).eval()\n",
    "\n",
    "if hasattr(vocoder, \"hifigan\") and hasattr(vocoder.hifigan, \"remove_weight_norm\"):\n",
    "    vocoder.hifigan.remove_weight_norm()\n",
    "elif hasattr(vocoder, \"remove_weight_norm\"):\n",
    "    vocoder.remove_weight_norm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, torch, torchaudio\n",
    "from pathlib import Path\n",
    "\n",
    "SR, N_FFT, HOP, WIN, N_MELS = 16_000, 800, 200, 800, 80\n",
    "\n",
    "@torch.no_grad()\n",
    "def vocode(mel_log: torch.Tensor) -> torch.Tensor:\n",
    "    if mel_log.dim() == 4:\n",
    "        mel_log = mel_log.squeeze(1)\n",
    "    wav = vocoder(mel_log.to(DEVICE))\n",
    "    wav = wav.squeeze(1)\n",
    "    wav = wav / wav.abs().amax(dim=1, keepdim=True).clamp_min(1e-6)\n",
    "    return wav.cpu()\n",
    "\n",
    "@torch.no_grad()\n",
    "def text_to_speech(text: str, tts, vae, sched, temp = 0.5, device=None):\n",
    "    device = device or next(tts.parameters()).device\n",
    "    ids = tts.word_enc.tokenize([text]).to(device)\n",
    "    dummy = torch.zeros(1, 1, 80, 80, device=device)\n",
    "    zshape = vae.encode(dummy)[0].shape\n",
    "    z = torch.randn(zshape, device=device) * temp\n",
    "    for t in reversed(range(sched.steps)):\n",
    "        eps = tts(z, ids, torch.tensor([t], device=device))\n",
    "        if eps.shape[-2:] != z.shape[-2:]:\n",
    "            eps = F.interpolate(eps, z.shape[-2:], mode=\"bilinear\")\n",
    "        z = sched.denoise_step(z, eps, t, sigma=0.0 if t < sched.steps * 0.2 else 1e-4)\n",
    "    mel_out = vae.decode(z)\n",
    "    mel_out = mel_out * mel_std + mel_mean\n",
    "    wav = vocode(mel_out)\n",
    "    return wav[0]      \n",
    "\n",
    "def slugify(text, length=16):\n",
    "    txt = re.sub(r\"\\s+\", \"_\", text.lower())\n",
    "    return re.sub(r\"[^\\w\\d_]+\", \"\", txt)[:length]\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tts_model = TTS_diffusion(\n",
    "    input_channels=32,\n",
    "    hidden_dims=128,\n",
    "    alphabet=\"абвгдеёжзийклмнопрстуфхцчшщъыьэюяАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ\",\n",
    "    emb_size_word=64,\n",
    "    emb_size_noise=32,\n",
    "    noise_steps=300,\n",
    "    max_word_size=256,\n",
    "    max_time_size=2048\n",
    ").to(DEVICE)\n",
    "\n",
    "trainer = TTS_Trainer(\n",
    "    model = tts_model,\n",
    "    vae = vae,\n",
    "    train_dl = train_dataloader,\n",
    "    val_dl = val_dataloader,\n",
    "    noise_steps = 300,\n",
    "    epochs = 30\n",
    ")\n",
    "\n",
    "Path(\"./samples\").mkdir(exist_ok=True)\n",
    "\n",
    "for epoch in range(trainer.epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{trainer.epochs}\")\n",
    "    trainer.train_loop(epoch=epoch)\n",
    "    val_mse, val_l2 = trainer.val_loop()\n",
    "    print(f\"ε-L2 val = {val_l2}  MSE = {val_mse}\")\n",
    "    trainer.draw_diffusion(f\"diff_ep{epoch + 1}.png\")\n",
    "    trainer.ema.eval()\n",
    "    for phrase in [\"привет\", \"здравствуйте\"]:\n",
    "        wav = text_to_speech(phrase, trainer.ema.module, trainer.vae, trainer.noise_sched, temp=0.5, device=DEVICE)\n",
    "        fn = f\"./samples/ep{epoch+1:04d}_{slugify(phrase)}.mp3\"\n",
    "        torchaudio.save(fn, wav.unsqueeze(0), SR)\n",
    "        print(\"OK\", fn)\n",
    "    torch.save(trainer.model.state_dict(), f\"TTS_Diffusion_11.{epoch + 1:02d}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(trainer.noise_sched.alpha_hat.cpu().numpy())\n",
    "plt.title(\"Alpha_hat Schedule\"); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.ema.eval()\n",
    "text = \"привет\"\n",
    "wav = text_to_speech(text, trainer.ema.module, trainer.vae, trainer.noise_sched, temp=0.7, device=DEVICE)\n",
    "\n",
    "wav = wav / wav.abs().max().clamp_min(1e-6)\n",
    "torchaudio.save(f\"sample_{slugify(text)}.wav\", wav.unsqueeze(0), SR)\n",
    "print(f\"test_{slugify(text)}.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02ee7ca7bdef4b4b87af083475c1676a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0917c668bf704493880fc06eba149bcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a94653d25fb4e39bf10968ae5539510",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5e212209cc264763a9c9d772784fc3ee",
      "value": 0
     }
    },
    "172ddc57d72a4b44815669efe43dbb3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_29aa757761714d59910d871b4cd9ad7b",
      "max": 3750,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8c94bbea11594888882c81a30b429207",
      "value": 3049
     }
    },
    "1a00c32513e94f0cade529efdc7970b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e2b8a9ea3b0471991562f13e0f09332": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "29aa757761714d59910d871b4cd9ad7b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29f4325df69048b38943cb4102e4b206": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37ee1c64b7ef452a852f821ab6298432": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_97b66af426c44b05a555dc6dcac2f2f2",
      "placeholder": "​",
      "style": "IPY_MODEL_fe81edd7112a4960a103d1309368d21f",
      "value": "  0%"
     }
    },
    "3e715d546530473a8a5af642b0067b07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_02ee7ca7bdef4b4b87af083475c1676a",
      "placeholder": "​",
      "style": "IPY_MODEL_923de72f80c74171a37067ef0477a8a0",
      "value": "train, loss = 1.841:  81%"
     }
    },
    "4b01e294e1da4a289a42967fb4f65e5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c8685e41c56d441ea9a54d7204faa9d9",
      "placeholder": "​",
      "style": "IPY_MODEL_80973a38f66b455b85780085b4623adb",
      "value": " 0/10 [00:00&lt;?, ?it/s]"
     }
    },
    "4d2bac776b774b59ad679f24e1a2f0e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5caa7b5e2c0145e78991da80720ee832": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d69f451fe04c4a41a18920fe4ce56f64",
      "placeholder": "​",
      "style": "IPY_MODEL_d67d302be41a482086e05db0c04e2ef5",
      "value": " 3049/3750 [03:17&lt;00:43, 15.99it/s]"
     }
    },
    "5e212209cc264763a9c9d772784fc3ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "745c665a5bb74a9490acb360bdfdd503": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_37ee1c64b7ef452a852f821ab6298432",
       "IPY_MODEL_0917c668bf704493880fc06eba149bcc",
       "IPY_MODEL_4b01e294e1da4a289a42967fb4f65e5b"
      ],
      "layout": "IPY_MODEL_29f4325df69048b38943cb4102e4b206"
     }
    },
    "7a94653d25fb4e39bf10968ae5539510": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80973a38f66b455b85780085b4623adb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8c94bbea11594888882c81a30b429207": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8de95e223bc347e598e6cf351e075a57": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "923de72f80c74171a37067ef0477a8a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "97b66af426c44b05a555dc6dcac2f2f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ebbd2e572b547898bf5ecde2d3ea9c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a827198306294285844ed8666c878998": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f8c86823b74a43259368748140f32d89",
       "IPY_MODEL_f8b65c5be65a4f70b18461d08c2e3c33",
       "IPY_MODEL_c60030bebf684f798c24c6115150b4c8"
      ],
      "layout": "IPY_MODEL_9ebbd2e572b547898bf5ecde2d3ea9c7"
     }
    },
    "c60030bebf684f798c24c6115150b4c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e8ea8438fcea49c898a034032db96e42",
      "placeholder": "​",
      "style": "IPY_MODEL_e5c6a52210244a4cbd8451145d15968b",
      "value": " 876/3750 [00:50&lt;02:00, 23.87it/s]"
     }
    },
    "c8685e41c56d441ea9a54d7204faa9d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb05fc7c458c4a44b7c75c09f83c16da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d67d302be41a482086e05db0c04e2ef5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d69f451fe04c4a41a18920fe4ce56f64": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5c6a52210244a4cbd8451145d15968b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e8ea8438fcea49c898a034032db96e42": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f052f2f296854f7fb0e85e3832bd7d25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3e715d546530473a8a5af642b0067b07",
       "IPY_MODEL_172ddc57d72a4b44815669efe43dbb3e",
       "IPY_MODEL_5caa7b5e2c0145e78991da80720ee832"
      ],
      "layout": "IPY_MODEL_1a00c32513e94f0cade529efdc7970b3"
     }
    },
    "f8b65c5be65a4f70b18461d08c2e3c33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d2bac776b774b59ad679f24e1a2f0e2",
      "max": 3750,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8de95e223bc347e598e6cf351e075a57",
      "value": 876
     }
    },
    "f8c86823b74a43259368748140f32d89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cb05fc7c458c4a44b7c75c09f83c16da",
      "placeholder": "​",
      "style": "IPY_MODEL_1e2b8a9ea3b0471991562f13e0f09332",
      "value": "0 epoch, train, loss = 0.089:  23%"
     }
    },
    "fe81edd7112a4960a103d1309368d21f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
