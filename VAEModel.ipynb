{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "xa_ko5MIt3JU"
   },
   "outputs": [],
   "source": [
    "import torch, torchvision, torchaudio, numpy as np, matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torchvision.utils as vutils\n",
    "import torch.backends.cudnn as cudnn\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device('cuda') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vzKsGBov1fl"
   },
   "source": [
    "# Конфигурирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "J8va9Ak-v7MW"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"dataset\": {\n",
    "        \"train\": {\n",
    "            \"table\": \"E:/data/train.csv\",\n",
    "            \"data\": \"E:/data/bare_data/\"\n",
    "        },\n",
    "        \"val\": {\n",
    "            \"table\": \"E:/data/val.csv\",\n",
    "            \"data\": \"E:/data/bare_data/\"\n",
    "        }\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"batch_size\": 32,\n",
    "        \"grad_acum\": 1,\n",
    "        \"dtype\": \"float32\",\n",
    "        'shuffle': True,\n",
    "        'pin_memory': True,\n",
    "    },\n",
    "    \"val\": {\n",
    "        \"batch_size\": 32,\n",
    "        \"grad_acum\": 1,\n",
    "        \"dtype\": \"float32\",\n",
    "        'shuffle': False,\n",
    "        'pin_memory': True,\n",
    "    },\n",
    "    \"vae\": {\n",
    "        \"freq\": 16000,\n",
    "        \"lenght\": 5,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"latent_size\": 128,\n",
    "        \"epochs\": 15,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"freq_scale\": 4,\n",
    "        \"time_scale\": 4,\n",
    "    },\n",
    "    \"utils\": {\n",
    "        \"n_fft\": 800,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wx0ZA-xpGWYD"
   },
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "P-aY_E4wFYlT"
   },
   "outputs": [],
   "source": [
    "mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate = 16000,\n",
    "    n_fft = 800,\n",
    "    hop_length = 200,     \n",
    "    win_length  = 800,\n",
    "    n_mels = 80,\n",
    ")\n",
    "\n",
    "def wav_to_mel(wav):\n",
    "    # wav: (1, L)\n",
    "    spec = mel_spec(wav)      \n",
    "    spec = torch.log(spec + 1e-6)\n",
    "    T = spec.shape[-1]\n",
    "    if T < 80:                    \n",
    "        spec = F.pad(spec, (0, 80 - T))\n",
    "    elif T > 80:                  \n",
    "        spec = spec[..., :80]\n",
    "    return spec  \n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, csv_file, audio_dir, target_sr=16000, length_sec=None, transform=None):\n",
    "        self.table = pd.read_csv(csv_file)\n",
    "        self.audio_dir = Path(audio_dir)\n",
    "        self.sr = target_sr\n",
    "        self.length = int(target_sr * length_sec) if length_sec else None\n",
    "        self.transform = transform\n",
    "\n",
    "    def load_wav(self, path):\n",
    "        wav, sr = torchaudio.load(path)\n",
    "        if wav.shape[0] > 1:\n",
    "            wav = wav.mean(dim=0, keepdim=True)\n",
    "        if sr != self.sr:\n",
    "            wav = torchaudio.functional.resample(wav, sr, self.sr)\n",
    "        return wav\n",
    "\n",
    "    def pad_trim(self, wav):\n",
    "        if self.length is None:\n",
    "            return wav\n",
    "        cur = wav.shape[-1]\n",
    "        if cur > self.length:\n",
    "            wav = wav[..., : self.length]\n",
    "        elif cur < self.length:\n",
    "            wav = torch.nn.functional.pad(wav, (0, self.length - cur))\n",
    "        return wav\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.table.iloc[idx]\n",
    "        rel_path = row[\"path\"]\n",
    "        text = row[\"sentence\"]\n",
    "        wav = self.load_wav(self.audio_dir / rel_path)\n",
    "        wav = self.pad_trim(wav).float()\n",
    "        mel = wav_to_mel(wav)\n",
    "\n",
    "        if self.transform:\n",
    "            mel = self.transform(mel)\n",
    "\n",
    "        return mel, text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.table)\n",
    "\n",
    "\n",
    "def audio_collate(batch):\n",
    "    mels, texts = zip(*batch)\n",
    "    return torch.stack(mels), list(texts)                 \n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "def build_dataloader(cfg, split, transform=None, workers=4, limit=10_000):\n",
    "    d  = cfg[\"dataset\"][split]\n",
    "    ds = AudioDataset(\n",
    "        d[\"table\"], d[\"data\"],\n",
    "        cfg[\"vae\"][\"freq\"], cfg[\"vae\"][\"lenght\"],\n",
    "        transform,\n",
    "    )\n",
    "    if limit and limit < len(ds):\n",
    "        idx = np.random.choice(len(ds), limit, replace=False)\n",
    "        ds = torch.utils.data.Subset(ds, idx)    \n",
    "    sampler = None\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size = cfg[split][\"batch_size\"],\n",
    "        shuffle = (split == \"train\"),\n",
    "        sampler = sampler,\n",
    "        num_workers = workers,\n",
    "        pin_memory = cfg[split][\"pin_memory\"],\n",
    "        collate_fn = audio_collate,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awbxL5qmOKXV"
   },
   "source": [
    "## Модель\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwbt6razf_wc"
   },
   "source": [
    "### Код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "EoUSmRETPz5g"
   },
   "outputs": [],
   "source": [
    "# Класс для перевода объекта в латентное пространство для упрощённой работы с ним, подаём объект -> получаем тензор меток на него, т.е. какими признаками он обладает и с помощью этого можем его сравнивать с другими и обучать модель.\n",
    "\n",
    "class VAE_Audio(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        # Слой получает признаки из исходного объекта\n",
    "        self.encoder_input = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.GELU(),\n",
    "            nn.Conv2d(64, 32, 3, 1, 1), nn.BatchNorm2d(32), nn.GELU(),\n",
    "        )\n",
    "        # Слой сжимает объект в латентное подпространство\n",
    "        self.encoder_squeeze = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, 3, 2, 1), nn.BatchNorm2d(32), nn.GELU(),\n",
    "            nn.Conv2d(32, 32, 3, 2, 1), nn.BatchNorm2d(32), nn.GELU(),\n",
    "        )\n",
    "        # Нужны, чтобы задать параметры Гауссовского распределения (mu - среднее, logvar - логарифм дисперсии)\n",
    "        self.encoder_mu = nn.Conv2d(32, 32, 1) # Набор меток, отвечающих за среднее значение признаков\n",
    "        self.encoder_logvar = nn.Conv2d(32, 32, 1) # Набор меток, отвечающих за то, как широко разбросаны признаки по латентному подпространству \n",
    "        \n",
    "        # Слои восстанавливают размерность до исходной\n",
    "        self.decoder_unsqueeze = nn.Sequential(\n",
    "                nn.ConvTranspose2d(32, 32, 3, 2, 1, output_padding=1), nn.BatchNorm2d(32), nn.GELU(),\n",
    "                nn.ConvTranspose2d(32, 32, 3, 2, 1, output_padding=1), nn.BatchNorm2d(32), nn.GELU(),\n",
    "        )\n",
    "        # Слой, отвечающий за выход: получаем объект, максимально похожий на исходный\n",
    "        self.decoder_output = nn.Sequential(\n",
    "                 nn.ConvTranspose2d(32, 16, 3, 1, 1), nn.GELU(),\n",
    "                 nn.BatchNorm2d(16),\n",
    "                 nn.ConvTranspose2d(16, 1, 3, 1, 1), \n",
    "        )\n",
    "    def encode(self, x): # Функция кодирования объекта в латентное подпространство (пространство меньшей размерности), получает параметры кодирования объекта (mu, logvar)\n",
    "        x = self.encoder_input(x)\n",
    "        x = self.encoder_squeeze(x)\n",
    "        mu = self.encoder_mu(x)\n",
    "        logvar = self.encoder_logvar(x)\n",
    "        return mu, logvar\n",
    "    def decode(self, x): # Декодирует объект в исходное пространство по набору признаков\n",
    "        x = self.decoder_unsqueeze(x)\n",
    "        x = self.decoder_output(x)\n",
    "        return x\n",
    "    def KLD_loss(self, mu, logvar, q=0.005): # Вычисляет Kullback-Leibler divergence (мера различия между двумя вероятностными распределениями) между предсказанным распределением и стандартным нормальным распределением\n",
    "        kld = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        kld = torch.clamp(kld, min=q)\n",
    "        return kld.mean()\n",
    "    def forward(self, x): # Полное прохождение через VAE\n",
    "        mu, logvar = self.encode(x)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        z = mu + std * torch.randn_like(std)\n",
    "        recon = self.decode(z)\n",
    "        return recon, z, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "k7gib70FRkcP"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "class AverageMeter:\n",
    "    def __init__(self,):\n",
    "        self.arr = []\n",
    "    def __call__(self, item, n=1):\n",
    "        if n<=1:\n",
    "            self.arr.extend([item])\n",
    "        else:\n",
    "            self.arr.extend([item]*n)\n",
    "    def __str__(self,) -> str:\n",
    "        return str(np.mean(np.array(self.arr)))\n",
    "    def zero(self,):\n",
    "        self.arr=[]\n",
    "\n",
    "class VAE_Trainer:\n",
    "    def __init__(self, model, train_dataloader, val_dataloader,):\n",
    "        self.model = model\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.tdl = train_dataloader\n",
    "        self.vdl = val_dataloader\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-3)\n",
    "        self.rec_loss = nn.L1Loss(reduction=\"mean\")\n",
    "        self.loss_meter = AverageMeter()\n",
    "\n",
    "    @staticmethod\n",
    "    def _show_example(model, loader, epoch, device):\n",
    "        mel, _ = next(iter(loader))\n",
    "        mel = mel.to(device)\n",
    "        with torch.no_grad():\n",
    "            recon, _, mu, logvar = model(mel[:1])\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(6, 2.5))\n",
    "        ax[0].imshow(mel[0, 0].cpu(), origin='lower', aspect='auto', cmap=\"magma\"); ax[0].set_title('orig')\n",
    "        ax[1].imshow(recon[0, 0].cpu(), origin='lower', aspect='auto', cmap=\"magma\"); ax[1].set_title(f'recon e{epoch}')\n",
    "        for a in ax: a.axis('off')\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "\n",
    "    def train_loop(self, epoch=0):\n",
    "        self.model.train()\n",
    "        self.loss_meter.zero()\n",
    "        pbar = tqdm(self.tdl, desc=f'train e{epoch}', leave=False)\n",
    "        for original_audio, _ in pbar:\n",
    "            original_audio = original_audio.to(self.device, non_blocking=True)\n",
    "            output, _, mu, logvar = self.model(original_audio)\n",
    "            recon = self.rec_loss(output, original_audio)\n",
    "            KLD = self.model.KLD_loss(mu, logvar)\n",
    "            beta = min(1.0, epoch / 20) * 0.5\n",
    "            loss = recon + beta * KLD\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.loss_meter(loss.item(), original_audio.size(0))\n",
    "            pbar.set_postfix(recon=f'{recon.item():.4f}', beta=f'{beta:.4f}', kld=f'{(beta*KLD).item():.6f}')\n",
    "        print(\"Train Loss = \" + self.loss_meter.__str__())\n",
    "\n",
    "    def val_loop(self, epoch=None):\n",
    "        self.model.eval()\n",
    "        self.loss_meter.zero()\n",
    "        mu_means, mu_stds = [], []\n",
    "        lv_means, lv_stds = [], []\n",
    "        for original_audio, _ in tqdm(self.vdl, desc='val'):\n",
    "            with torch.no_grad():\n",
    "                original_audio = original_audio.to(self.device, non_blocking=True)\n",
    "                output, _, mu, logvar = self.model(original_audio)\n",
    "                mu_means.append(mu.mean().item())\n",
    "                mu_stds.append(mu.std().item())\n",
    "                lv_means.append(logvar.mean().item())\n",
    "                lv_stds.append(logvar.std().item())\n",
    "                loss = self.rec_loss(output, original_audio)\n",
    "                self.loss_meter(loss.item(), original_audio.size(0))\n",
    "        print(f\"Validation loss = {self.loss_meter}\")\n",
    "        print(f\"Encoder μ:   mean = {sum(mu_means)/len(mu_means)}, std = {sum(mu_stds)/len(mu_stds)}\")\n",
    "        print(f\"Encoder logσ²: mean = {sum(lv_means)/len(lv_means)}, std = {sum(lv_stds)/len(lv_stds)}\")\n",
    "        self._show_example(self.model, self.vdl, epoch, self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "kryABkVUU-1G"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_37968\\685688704.py:22: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.table = pd.read_csv(csv_file)\n"
     ]
    }
   ],
   "source": [
    "cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "train_dataloader = build_dataloader(config, \"train\", workers=0, limit=1000)\n",
    "val_dataloader = build_dataloader(config, \"val\", workers=0, limit=None)\n",
    "vae = VAE_Audio().to(device)\n",
    "trainer = VAE_Trainer(vae, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch shape: torch.Size([32, 1, 80, 80])\n",
      "CPU times: total: 1.17 s\n",
      "Wall time: 174 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mel, _ = next(iter(train_dataloader))\n",
    "print(\"batch shape:\", mel.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abbc567e47034a3a868f17e8fefeb92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train e0:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 7.561288307189941\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56dbddfea1c445afb4a93d7e5dc5a945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/532 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "    trainer.train_loop(epoch)\n",
    "    trainer.val_loop(epoch)\n",
    "    if ((epoch + 1) % 5 == 0):\n",
    "        torch.save(vae.state_dict(), f\"TTSVAE_v10.{epoch + 1}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "CsovPdB3f7PC"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
